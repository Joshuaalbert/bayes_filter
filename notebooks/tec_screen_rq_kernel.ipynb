{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "import GPyOpt\nimport GPy\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport astropy.time as at\nimport astropy.coordinates as ac\nimport astropy.units as au\nimport gpflow as gp\nfrom bayes_filter.datapack import DataPack\nfrom bayes_filter.misc import make_coord_array, get_screen_directions,maybe_create_posterior_solsets\nfrom bayes_filter.coord_transforms import ITRSToENUWithReferences\nfrom bayes_filter.settings import dist_type, angle_type\nfrom bayes_filter.kernels import DTECIsotropicTimeGeneral\nfrom bayes_filter.plotting import plot_vornoi_map\nfrom bayes_filter import logging\nimport pylab as plt\nfrom scipy.special import erf\n\nfrom functools import reduce\nimport warnings\nimport itertools\n\nimport tensorflow as tf\nimport numpy as np\nimport gpflow as gp\n\nfrom gpflow import transforms\nfrom gpflow import settings\n\nfrom gpflow.params import Parameter, Parameterized, ParamList\nfrom gpflow.decors import params_as_tensors, autoflow\nfrom bayes_filter.rayintegral_kernels import TrapezoidKernel, RQ, EQ, M52\nfrom bayes_filter.coord_transforms import ITRSToENUWithReferences_v2\n\nfloat_type \u003d settings.float_type\n\n\nclass DTECKernel(gp.kernels.Kernel):\n    def __init__(self, input_dim, amplitude\u003d1., lengthscales\u003d10.0, scale_mixture_rate \u003d 1./3.,\n                 a\u003d200., b\u003d100., resolution\u003d10, ref_direction\u003dNone, ref_location\u003dNone,\n                 active_dims\u003dNone, fed_kernel\u003d\u0027RQ\u0027, obs_type\u003d\u0027DTEC\u0027, name\u003dNone):\n        \"\"\"\n        - input_dim is the dimension of the input to the kernel\n        - variance is the (initial) value for the variance parameter\n        - lengthscales is the initial value for the lengthscales parameter\n          defaults to 1.0 (ARD\u003dFalse) or np.ones(input_dim) (ARD\u003dTrue).\n        - active_dims is a list of length input_dim which controls which\n          columns of X are used.\n        \"\"\"\n        super().__init__(input_dim, active_dims, name\u003dname)\n        self.amplitude \u003d Parameter(amplitude, transform\u003dtransforms.positiveRescale(amplitude),\n                                  dtype\u003dsettings.float_type)\n        # (3,)\n        self.lengthscales \u003d Parameter(lengthscales, transform\u003dtransforms.positiveRescale(lengthscales),\n                                      dtype\u003dsettings.float_type)\n        self.scale_mixture_rate \u003d Parameter(scale_mixture_rate, transform\u003dtransforms.positiveRescale(scale_mixture_rate),\n                                      dtype\u003dsettings.float_type)\n\n        self.a \u003d Parameter(a, transform\u003dtransforms.positiveRescale(a),\n                           dtype\u003dsettings.float_type)\n        self.b \u003d Parameter(b, transform\u003dtransforms.positiveRescale(b),\n                           dtype\u003dsettings.float_type)\n        self.resolution \u003d resolution\n        self.obs_type \u003d obs_type\n        self.fed_kernel \u003d fed_kernel\n        self.ref_direction \u003d Parameter(ref_direction, dtype\u003dfloat_type)\n        self.ref_location \u003d Parameter(ref_location, dtype\u003dfloat_type)\n\n    @params_as_tensors\n    def Kdiag(self, X, presliced\u003dFalse):\n        if not presliced:\n            X, _ \u003d self._slice(X, None)\n        return tf.diag_part(self.K(X, None))\n\n    @params_as_tensors\n    def K(self, X, X2\u003dNone, presliced\u003dFalse):\n\n        if not presliced:\n            X, X2 \u003d self._slice(X, X2)\n\n        int_kern \u003d None\n        if self.fed_kernel \u003d\u003d \u0027RQ\u0027:\n            int_kern \u003d RQ([self.amplitude, self.lengthscales, self.scale_mixture_rate])\n        if self.fed_kernel \u003d\u003d \u0027EQ\u0027:\n            int_kern \u003d EQ([self.amplitude, self.lengthscales])\n        if self.fed_kernel \u003d\u003d \u0027M52\u0027:\n            int_kern \u003d M52([self.amplitude, self.lengthscales])\n\n        kern \u003d TrapezoidKernel(int_kern, self.resolution, self.a, self.b,\n                               ref_direction\u003dself.ref_direction, ref_location\u003dself.ref_location,\n                               obs_type\u003dself.obs_type, ionosphere_type\u003d\u0027flat\u0027)\n\n        return kern.K(X,X2)\n\nimport tensorflow as tf\n\nfrom gpflow import likelihoods\nfrom gpflow import settings\n\nfrom gpflow.conditionals import base_conditional\nfrom gpflow.params import DataHolder\nfrom gpflow.decors import params_as_tensors\nfrom gpflow.decors import name_scope\nfrom gpflow.logdensities import multivariate_normal\n\nfrom gpflow.models.model import GPModel\n\n\nclass HGPR(GPModel):\n    r\"\"\"\n    Gaussian Process Regression.\n    This is a vanilla implementation of GP regression with a Gaussian\n    likelihood. In this case inference is exact, but costs O(N^3). This means\n    that we can compute the predictive distributions (predict_f, predict_y) in\n    closed-form, as well as the marginal likelihood, which we use to estimate\n    (optimize) the kernel parameters.\n\n    Multiple columns of Y are treated independently, using the same kernel.\n    The log likelihood of this model is sometimes referred to as the\n    \u0027marginal log likelihood\u0027, and is given by\n    .. math::\n       \\log p(\\mathbf y | \\mathbf f) \u003d \\mathcal N(\\mathbf y | 0, \\mathbf K + \\sigma_n \\mathbf I)\n    \"\"\"\n\n    def __init__(self, X, Y, Y_var, kern, mean_function\u003dNone, parallel_iterations\u003d1, name\u003dNone):\n        \"\"\"\n        X is a data matrix, size N x D\n        Y is a data matrix, size N x R\n        kern, mean_function are appropriate GPflow objects\n        name is a string which can be used to name this model (useful for handling multiple models on one tf.graph)\n        \"\"\"\n        likelihood \u003d likelihoods.Gaussian()\n        # M, D\n        X \u003d DataHolder(X)\n        # T, M\n        Y \u003d DataHolder(Y)\n        num_latent \u003d Y.shape[0]\n        GPModel.__init__(self, X\u003dX, Y\u003dY, kern\u003dkern, likelihood\u003dlikelihood,\n                         mean_function\u003dmean_function, num_latent\u003dnum_latent, name\u003dname)\n        self.Y_var \u003d DataHolder(Y_var)\n        self.parallel_iterations \u003d parallel_iterations\n\n    @name_scope(\u0027likelihood\u0027)\n    @params_as_tensors\n    def _build_likelihood(self):\n        r\"\"\"\n        Construct a tensorflow function to compute the likelihood.\n            \\log p(Y | theta).\n        \"\"\"\n        # M,M + T, M, M -\u003e T, M, M\n        K \u003d self.kern.K(self.X)\n        Y_std \u003d tf.math.sqrt(self.Y_var)\n\n        def single_marginal(args):\n            y, y_std \u003d args\n            M \u003d tf.shape(y_std)[0]\n            K_sigma \u003d K / (y_std[:, None] * y_std[None, :]) + tf.linalg.eye(M, dtype\u003dfloat_type)\n            L \u003d tf.linalg.cholesky(K_sigma)\n            y /\u003d y_std\n            A \u003d tf.linalg.triangular_solve(L, y[:, None])\n            maha \u003d -0.5 * tf.reduce_sum(tf.math.square(A))\n            # (sigma.L.L^T.sigma^T)^-1 \u003d sigma^-T.L^-T.L^-1 sigma^-1\n            # log(0) + log(infty) -\u003e 0\n            logdetL \u003d tf.reduce_sum(\n                tf.math.log(\n                    tf.where(tf.equal(y_std, tf.constant(np.inf, float_type)),\n                             tf.ones_like(y_std),\n                             tf.linalg.diag_part(L) * y_std)\n                )\n            )\n            constant \u003d 0.5 * np.log(np.sqrt(2. * np.pi)) * tf.cast(M, float_type)\n            return maha - logdetL - constant\n\n        logpdf \u003d tf.map_fn(single_marginal, [self.Y, Y_std], float_type, parallel_iterations\u003dself.parallel_iterations)\n\n        return tf.reduce_sum(logpdf)\n\n    @name_scope(\u0027predict\u0027)\n    @params_as_tensors\n    def _build_predict(self, Xnew, full_cov\u003dFalse):\n        \"\"\"\n        Xnew is a data matrix, the points at which we want to predict.\n        This method computes\n            p(F* | Y)\n        where F* are points on the GP at Xnew, Y are noisy observations at X.\n        \"\"\"\n\n        # T,M,M\n        Kmm \u003d self.kern.K(self.X)\n        # M,N\n        Kmn \u003d self.kern.K(self.X, Xnew)\n        Knn \u003d self.kern.K(Xnew) if full_cov else self.kern.Kdiag(Xnew)\n\n        Y_std \u003d tf.math.sqrt(self.Y_var)\n\n        def single_predict_f(args):\n            y, y_std \u003d args\n            M \u003d tf.shape(y_std)[0]\n            # M,M\n            Kmm_sigma \u003d Kmm / (y_std[:, None] * y_std[None, :]) + tf.linalg.eye(M, dtype\u003dfloat_type)\n            # (sigma.L.L^T.sigma^T)^-1 \u003d sigma^-T.L^-T.L^-1 sigma^-1\n            # M,M\n            L \u003d tf.linalg.cholesky(Kmm_sigma)\n            # M,N\n            A \u003d tf.linalg.triangular_solve(L, Kmn / y_std[:, None])\n            # N\n            post_mean \u003d tf.matmul(A, tf.linalg.triangular_solve(L, y[:, None] / y_std[:, None]), transpose_a\u003dTrue)[:, 0]\n            if full_cov:\n                # N,N\n                post_cov \u003d Knn - tf.matmul(A, A, transpose_a\u003dTrue)\n            else:\n                # N\n                # sum_k A[k,i]A[k,j]\n                # N + T,N -\u003e T,N\n                post_cov \u003d Knn - tf.reduce_mean(tf.math.square(A), axis\u003d0)\n            return [post_mean, post_cov]\n\n        post_mean, post_cov \u003d tf.map_fn(single_predict_f, [self.Y, Y_std], [float_type, float_type],\n                                        parallel_iterations\u003dself.parallel_iterations)\n        return post_mean, post_cov\n\n\nreinout_datapack \u003d \u0027/home/albert/lofar1_1/imaging/data/P126+65_compact_raw/P126+65_full_compact_raw_v8.h5\u0027\nscreen_directions \u003d get_screen_directions(\u0027/home/albert/ftp/image.pybdsm.srl.fits\u0027, flux_limit\u003d0.05, max_N\u003d250,min_spacing_arcmin\u003d1.)\nmaybe_create_posterior_solsets(reinout_datapack, \u0027sol000\u0027, posterior_name\u003d\u0027posterior\u0027, screen_directions\u003dscreen_directions)\n\ndatapack \u003d DataPack(reinout_datapack, readonly\u003dFalse)\nselect \u003d dict(pol\u003dslice(0, 1, 1),\n              ant\u003dslice(None, None, 1),\n              dir\u003dslice(None, None, 1),\n              time\u003dslice(None, None, 1))\ndatapack.current_solset \u003d \u0027data_posterior\u0027\ndatapack.select(**select)\n\ntec, _ \u003d datapack.tec\ntec[:, 14, ...] \u003d 0.\ndatapack.tec \u003d tec\n\n# Nd, Na, Nt\nreinout_flags \u003d np.load(\u0027/home/albert/lofar1_1/imaging/data/flagsTECBay.npy\u0027)\nreinout_flags \u003d np.where(reinout_flags \u003d\u003d 1., np.inf, 1.)  # uncertainty in mTECU\ndatapack.weights_tec \u003d reinout_flags\n\nrecalculate_weights \u003d False\nant_cutoff \u003d 0.15\nref_dir_idx \u003d 14\nblock_size \u003d 40\n\ndatapack \u003d DataPack(reinout_datapack, readonly\u003dTrue)\nselect \u003d dict(pol\u003dslice(0, 1, 1),\n              ant\u003dslice(None, None, 1),\n              dir\u003dslice(None, None, 1),\n              time\u003dslice(None, None, 1))\ndatapack.current_solset \u003d \u0027data_posterior\u0027\ndatapack.select(**select)\naxes \u003d datapack.axes_tec\n\n###\n# cutoff dist for antennas\nants, antennas \u003d datapack.get_antennas(axes[\u0027ant\u0027])\nXa \u003d antennas.cartesian.xyz.to(dist_type).value.T\nXa_screen \u003d Xa\nNa_screen \u003d Xa.shape[0]\n\nref_ant \u003d Xa[0, :]\nNa \u003d len(antennas)\nkeep \u003d []\n\nfor i in range(0, Na):\n    if np.all(np.linalg.norm(Xa[i:i + 1, :] - Xa[keep, :], axis\u003d1) \u003e ant_cutoff):\n        keep.append(i)\n\nlogging.info(\"Training on {} antennas\".format(len(keep)))\n\n###\n# Load data\n\n\nselect[\u0027ant\u0027] \u003d keep\ndatapack.select(**select)\ntec, axes \u003d datapack.tec\ntec -\u003d tec[:, ref_dir_idx:ref_dir_idx + 1, :, :]\ntec_uncert, _ \u003d datapack.weights_tec\n\n# Nd, Na, Nt -\u003e Nt, Nd, Na\ntec \u003d tec[0, ...].transpose((2, 0, 1))\ntec_uncert \u003d tec_uncert[0, ...].transpose((2, 0, 1))\n\nNt, Nd, Na \u003d tec.shape\n_, times \u003d datapack.get_times(axes[\u0027time\u0027])\nXt \u003d (times.mjd * 86400.)[:, None]\n_, directions \u003d datapack.get_directions(axes[\u0027dir\u0027])\nXd \u003d np.stack([directions.ra.to(angle_type).value, directions.dec.to(angle_type).value], axis\u003d1)\nref_dir \u003d Xd[ref_dir_idx, :]\n_, antennas \u003d datapack.get_antennas(axes[\u0027ant\u0027])\nXa \u003d antennas.cartesian.xyz.to(dist_type).value.T\n\ndatapack.current_solset \u003d \u0027screen_posterior\u0027\ndatapack.select(**select)\naxes \u003d datapack.axes_tec\n_, screen_directions \u003d datapack.get_directions(axes[\u0027dir\u0027])\nXd_screen \u003d np.stack([screen_directions.ra.to(angle_type).value, screen_directions.dec.to(angle_type).value], axis\u003d1)\nNd_screen \u003d Xd_screen.shape[0]\n# Xd_screen \u003d np.stack([np.random.uniform(-6*np.pi/180., 6.*np.pi/180.,size\u003dNd_screen) + directions.ra.to(angle_type).value.mean(),\n#                       np.random.uniform(-6*np.pi/180., 6.*np.pi/180.,size\u003dNd_screen) + directions.dec.to(angle_type).value.mean()],axis\u003d1)\n\nimport os\n\noutput_folder \u003d \u0027./screen_figs_2\u0027\nos.makedirs(output_folder, exist_ok\u003dTrue)\n\n\ndef get_outliers():\n    global Y_var, model, ystar, varstar, stdstar, detection\n    ###\n    # First outlier filter\n    Y_var \u003d 10. ** 2 * np.ones_like(Y)\n    model \u003d HGPR(X, Y, Y_var, kern)\n    ystar, varstar \u003d model.predict_f(X)\n    stdstar \u003d np.sqrt(varstar)\n    outliers \u003d np.abs(ystar - Y) \u003e 10.\n    cdf_outliers \u003d 0.5 * (1. + erf((Y[outliers] - ystar[outliers]) / stdstar[outliers] / np.sqrt(2.)))\n    print(np.mean(cdf_outliers), np.std(cdf_outliers))\n    cdf \u003d 0.5 * (1. + erf((Y - ystar) / stdstar / np.sqrt(2.)))\n    detection \u003d cdf \u003e np.mean(cdf_outliers)\n    detection \u003d outliers\n    mask \u003d np.logical_not(detection)\n    logging.info(\"First round of filtering: {} outliers\".format(detection.sum()))\n    Y_var \u003d np.where(detection, np.inf, 1.)\n    ###\n    # Refined outlier filter\n    model \u003d HGPR(X, Y, Y_var, kern)\n    ystar, varstar \u003d model.predict_f(X)\n    stdstar \u003d np.sqrt(varstar)\n    outliers \u003d np.abs(ystar - Y) \u003e 10.\n    cdf_outliers \u003d 0.5 * (1. + erf((Y[outliers] - ystar[outliers]) / stdstar[outliers] / np.sqrt(2.)))\n    print(np.mean(cdf_outliers), np.std(cdf_outliers))\n    cdf \u003d 0.5 * (1. + erf((Y - ystar) / stdstar / np.sqrt(2.)))\n    detection \u003d cdf \u003e np.mean(cdf_outliers)\n    detection \u003d outliers\n    mask \u003d np.logical_not(detection)\n    logging.info(\"Second round of filtering: {} outliers\".format(detection.sum()))\n\n\nwith tf.device(\"/device:CPU:0\"):\n    with tf.Session(graph\u003dtf.Graph()) as sess:\n\n        kern \u003d DTECKernel(6, amplitude\u003d50., lengthscales\u003d10.0,\n                          a\u003d160., b\u003d100., resolution\u003d8, scale_mixture_rate\u003d1./3.,\n                          fed_kernel\u003d\u0027RQ\u0027, obs_type\u003d\u0027DDTEC\u0027, ref_location\u003d[0.,0.,0.],\n                          ref_direction\u003d[0.,0.,1.])\n\n\n        posterior_screen_mean \u003d []\n        posterior_screen_std \u003d []\n        outlier_masks \u003d []\n\n        coord_transform \u003d ITRSToENUWithReferences_v2(ref_ant, ref_dir, ref_ant)\n        coords, ref_ants, ref_dirs \u003d sess.run(coord_transform(Xt, Xd, Xa))\n        coords_screen, _, _ \u003d sess.run(coord_transform(Xt, Xd_screen, Xa_screen))\n\n        for t in range(0, Nt, block_size):\n            start \u003d t\n            stop \u003d min(Nt, t + block_size)\n            mid_time \u003d start + (stop - start) // 2\n\n            kern.ref_location \u003d ref_ants[mid_time,:]\n            kern.ref_direction \u003d ref_dirs[mid_time, :]\n\n            X_screen \u003d coords_screen[start:stop,:,:,:].reshape((-1, 6))\n            X \u003d coords[start:stop, :, :, :].reshape((-1, 6))\n\n            # T, N\n            Y \u003d tec[start:stop, :, :].reshape((stop - start, -1))\n            # T, N\n            detection \u003d tec_uncert[start:stop, :, :].reshape((stop - start, -1))\n            detection \u003d np.where(detection \u003d\u003d np.inf, True, False)\n\n            if recalculate_weights:\n                get_outliers()\n\n            ###\n            # Predict\n            logging.info(\"Predicting with {} outliers\".format(detection.sum()))\n            Y_var \u003d np.where(detection, np.inf, 1.0)\n            model \u003d HGPR(X, Y, Y_var, kern, parallel_iterations\u003d10)\n            logging.info(\"Index {} -\u003e training hyperparams\".format(t))\n\n            logging.info(\"Doing bayesian optimisation\")\n            space \u003d [{\u0027name\u0027: \u0027amplitude\u0027, \u0027type\u0027: \u0027continuous\u0027, \u0027domain\u0027: (1., 160.)},\n                     {\u0027name\u0027: \u0027lengthscale\u0027, \u0027type\u0027: \u0027continuous\u0027, \u0027domain\u0027: (1., 30.)},\n                     {\u0027name\u0027: \u0027a\u0027, \u0027type\u0027: \u0027continuous\u0027, \u0027domain\u0027: (100., 400.)},\n                    {\u0027name\u0027: \u0027inv_scale_mixture_rate\u0027, \u0027type\u0027: \u0027continuous\u0027, \u0027domain\u0027: (0.5, 6.)}\n            ]\n\n            #                    {\u0027name\u0027: \u0027wild\u0027,\u0027type\u0027:\u0027discrete\u0027, \u0027domain\u0027:(0,1)}]\n            feasible_region \u003d GPyOpt.Design_space(space\u003dspace)\n            initial_design \u003d GPyOpt.experiment_design.initial_design(\u0027random\u0027, feasible_region, 30)\n            initial_design \u003d np.array([[kern.amplitude.value, kern.lengthscales.value,\n                                        kern.a.value, 1./kern.scale_mixture_rate.value]] + list(initial_design))\n\n\n            def opt_func(args):\n                amplitude, lengthscale, a, inv_scale_mixture_rate \u003d args[0, 0], args[0, 1], args[0, 2], args[0,3]\n                kern.lengthscales \u003d lengthscale  # if wild \u003d\u003d 0 else lengthscale*10.\n                kern.amplitude \u003d amplitude\n                kern.a \u003d a\n                kern.scale_mixture_rate \u003d 1./inv_scale_mixture_rate\n                lml \u003d model.compute_log_likelihood()\n                return -lml\n\n\n            # --- CHOOSE the objective\n            objective \u003d GPyOpt.core.task.SingleObjective(opt_func)\n\n            # --- CHOOSE the model type\n            bo_model \u003d GPyOpt.models.GPModel(exact_feval\u003dTrue, optimize_restarts\u003d10, verbose\u003dFalse)\n\n            # --- CHOOSE the acquisition optimizer\n            aquisition_optimizer \u003d GPyOpt.optimization.AcquisitionOptimizer(feasible_region)\n\n            # --- CHOOSE the type of acquisition\n            acquisition \u003d GPyOpt.acquisitions.AcquisitionEI(bo_model, feasible_region, optimizer\u003daquisition_optimizer)\n\n            # --- CHOOSE a collection method\n            evaluator \u003d GPyOpt.core.evaluators.Sequential(acquisition)\n            bo \u003d GPyOpt.methods.ModularBayesianOptimization(bo_model, feasible_region, objective, acquisition,\n                                                            evaluator, initial_design)\n            # --- Stop conditions\n            max_time \u003d None\n            max_iter \u003d 10\n            tolerance \u003d 1e-5  # distance between two consecutive observations\n\n            # Run the optimization\n            bo.run_optimization(max_iter\u003dmax_iter, max_time\u003dmax_time, eps\u003dtolerance, verbosity\u003dTrue)\n            amplitude, lengthscale, a, inv_scale_mixture_rate \u003d bo.x_opt\n            kern.lengthscales \u003d lengthscale  # if wild \u003d\u003d 0 else lengthscale*10.\n            kern.amplitude \u003d amplitude\n            kern.a \u003d a\n            kern.scale_mixture_rate \u003d 1./inv_scale_mixture_rate\n\n            #            logging.info(\"Trying multiple random initialisation...\")\n            #\n            #            for _ in range(30):\n            #                s \u003d np.random.uniform(1.,7.)\n            #                l \u003d np.random.uniform(2., 25.)\n            #                kern.lengthscales \u003d l\n            #                kern.variance \u003d s**2\n            #                lml \u003d model.compute_log_likelihood()\n            #                if lml \u003e best_log_marginal:\n            #                    best_hyperparams.append([s,l])\n            #                    best_log_marginal \u003d lml\n            #                    logging.info(\"Found good combo ({}): amp {} lengthscale {}\".format(lml, s,l))\n            #            kern.lengthscales \u003d best_hyperparams[-1][1]\n            #            kern.variance \u003d best_hyperparams[-1][0]**2\n            #            gp.train.ScipyOptimizer().minimize(model)\n\n            logging.info(str(kern.read_trainables()))\n            logging.info(\"Done index {} -\u003e training hyperparams\".format(t))\n            Y_var \u003d np.where(detection, np.inf, 1.)\n            model \u003d HGPR(X, Y, Y_var, kern, parallel_iterations\u003d10)\n\n            logging.info(\"Predicting screen from {} to {}\".format(start, stop))\n            predict_batch_size \u003d 3000\n            ystar, varstar \u003d [], []\n            for i in range(0, X_screen.shape[0], predict_batch_size):\n                start_ \u003d i\n                stop_ \u003d min(i + predict_batch_size, X_screen.shape[0])\n                ystar_, varstar_ \u003d model.predict_f(X_screen[start_:stop_, :])\n                ystar.append(ystar_)\n                varstar.append(varstar_)\n            ystar \u003d np.concatenate(ystar, axis\u003d1)\n            varstar \u003d np.concatenate(varstar, axis\u003d1)\n            logging.info(\"Done predicting screen from {} to {}\".format(start, stop))\n\n            ystar \u003d ystar.reshape((stop - start, Nd_screen, Na_screen))\n            stdstar \u003d np.sqrt(varstar).reshape((stop - start, Nd_screen, Na_screen))\n\n            posterior_screen_mean.append(ystar)\n            posterior_screen_std.append(stdstar)\n\n            # ###\n            # # plot\n            # ra \u003d Xd[:, 0] * 180 / np.pi\n            # dec \u003d Xd[:, 1] * 180. / np.pi\n            points \u003d Xd_screen * 180. / np.pi\n            # print(\"Plotting {}\".format(start))\n            # for a in range(Na_screen):\n            #\n            #     vmin \u003d ystar[0, :, a].min()\n            #     vmax \u003d ystar[0, :, a].max()\n            #     plot_vornoi_map(points, ystar[0, :, a], norm\u003dplt.Normalize(vmin, vmax), cmap\u003dplt.cm.gist_rainbow)\n            #     if a in keep:\n            #         ka \u003d keep.index(a)\n            #         detection_mask \u003d detection.reshape((stop - start, Nd, Na))[0, :, ka]\n            #         plt.scatter(ra, dec, s\u003d30., c\u003dtec[start, :, ka], cmap\u003dplt.cm.gist_rainbow, vmin\u003dvmin, vmax\u003dvmax,\n            #                     edgecolors\u003d\u0027black\u0027, zorder\u003d18)\n            #         plt.scatter(ra[detection_mask], dec[detection_mask], s\u003d100., c\u003dtec[start, detection_mask, ka],\n            #                     cmap\u003dplt.cm.gist_rainbow, vmin\u003dvmin, vmax\u003dvmax, edgecolors\u003d\u0027black\u0027, zorder\u003d19)\n            #     plt.title(\"{} vmin:{:.2f} vmax:{:.2f}\\n{}\".format(ants[a], vmin, vmax, \"\\n\".join(\n            #         [\"{} : {:.2f}\".format(key, value) for key, value in kern.read_trainables().items()])))\n            #     plt.xlim(ra.max() + 1, ra.min() - 1)\n            #     plt.ylim(dec.min() - 1, dec.max() + 1)\n            #     plt.tight_layout()\n            #     plt.savefig(os.path.join(output_folder, \u0027fig-{:03d}-{:02d}.png\u0027.format(start, a)))\n            #     plt.close(\u0027all\u0027)\n        #         outlier_masks.append(detection.reshape((stop-start, Nd, Na)))\n        posterior_screen_mean \u003d np.concatenate(posterior_screen_mean, axis\u003d0).transpose((1, 2, 0))[None, :, :, :]\n        posterior_screen_std \u003d np.concatenate(posterior_screen_std, axis\u003d0).transpose((1, 2, 0))[None, :, :, :]\n    #     outlier_masks \u003d np.concatenate(outlier_masks,axis\u003d0).transpose((1,2,0))[None, :,:,:]\n\nlogging.info(\"Storing\")\ndatapack \u003d DataPack(reinout_datapack, readonly\u003dFalse)\nselect \u003d dict(pol\u003dslice(0, 1, 1),\n              ant\u003dslice(None, None, 1),\n              dir\u003dslice(None, None, 1),\n              time\u003dslice(None, None, 1))\ndatapack.current_solset \u003d \u0027screen_posterior\u0027\ndatapack.select(**select)\ndatapack.tec \u003d posterior_screen_mean\ndatapack.weights_tec \u003d posterior_screen_std\nlogging.info(\"Done\")\n\n# %%\n\n# %%\n\n\n# %%\nselect[\u0027ant\u0027] \u003d None\nkeep \u003d list(range(62))\ndatapack.select(**select)\ndatapack.current_solset \u003d \u0027data_posterior\u0027\ntec, axes \u003d datapack.tec\ntec -\u003d tec[:, ref_dir_idx:ref_dir_idx + 1, :, :]\ntec_uncert, _ \u003d datapack.weights_tec\n\n_, direction \u003d datapack.get_directions(axes[\u0027dir\u0027])\nra \u003d directions.ra.deg\ndec \u003d directions.dec.deg\n\nshape \u003d posterior_screen_mean.shape\ndetection \u003d np.where(tec_uncert \u003d\u003d np.inf, True, False)\nfor start in range(0, shape[-1], 20):\n    print(\"Plotting {}\".format(start))\n    for a in range(Na_screen):\n        detection_mask \u003d detection[0, :, a, start]\n\n        vmin \u003d tec[0, np.logical_not(detection_mask), a, start].min()\n        vmax \u003d tec[0, np.logical_not(detection_mask), a, start].max()\n        plot_vornoi_map(points, posterior_screen_mean[0, :, a, start], norm\u003dplt.Normalize(vmin, vmax),\n                        cmap\u003dplt.cm.gist_rainbow)\n        if a in keep:\n            ka \u003d keep.index(a)\n\n            plt.scatter(ra, dec, s\u003d30., c\u003dtec[0, :, ka, start], cmap\u003dplt.cm.gist_rainbow, vmin\u003dvmin, vmax\u003dvmax,\n                        edgecolors\u003d\u0027black\u0027, zorder\u003d18)\n            plt.scatter(ra[detection_mask], dec[detection_mask], s\u003d100., c\u003dtec[0, detection_mask, ka, start],\n                        cmap\u003dplt.cm.gist_rainbow, vmin\u003dvmin, vmax\u003dvmax, edgecolors\u003d\u0027black\u0027, zorder\u003d19)\n        plt.title(\"{} vmin:{:.2f} vmax:{:.2f}\".format(ants[a], vmin, vmax))\n        plt.xlim(ra.max() + 1, ra.min() - 1)\n        plt.ylim(dec.min() - 1, dec.max() + 1)\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_folder, \u0027fig-{:03d}-{:02d}.png\u0027.format(start, a)))\n        plt.close(\u0027all\u0027)\n# %%\nres_datapack \u003d DataPack(\u0027/net/rijn/data2/rvweeren/P126+65_recall/ClockTEC/P126+65_full_compact_ampphasesmoothed.h5\u0027,\n                        readonly\u003dTrue)\n# %%\nres_datapack.current_solset \u003d \u0027sol000\u0027\nselect \u003d dict(pol\u003dslice(0, 1, 1),\n              ant\u003dslice(None, None, 1),\n              dir\u003dslice(None, None, 1),\n              time\u003dslice(None, None, 1))\nres_datapack.select(**select)\ncal_phase, axes \u003d res_datapack.phase\ncal_amp, _ \u003d res_datapack.amplitude\n_, cal_directions \u003d res_datapack.get_directions(axes[\u0027dir\u0027])\n# %%\ndatapack.current_solset \u003d \u0027sol000\u0027\ndatapack.select(**select)\nphase, _ \u003d datapack.phase\n\ndatapack.current_solset \u003d \u0027screen_posterior\u0027\ndatapack.select(**select)\naxes \u003d datapack.axes_phase\nantenna_labels, _ \u003d datapack.get_antennas(axes[\u0027ant\u0027])\npatch_names, _ \u003d datapack.get_directions(axes[\u0027dir\u0027])\npols, _ \u003d datapack.get_pols(axes[\u0027pol\u0027])\n\n_, freqs \u003d datapack.get_freqs(axes[\u0027freq\u0027])\n_, screen_directions \u003d datapack.get_directions(axes[\u0027dir\u0027])\ntec, _ \u003d datapack.tec\nphase_posterior \u003d tec[..., None, :] * -8.4479e6 / freqs[:, None] + phase[:, ref_dir_idx:ref_dir_idx+1, ...]\n\n# %%\ncal_Xd \u003d np.stack([cal_directions.ra.deg, cal_directions.dec.deg], axis\u003d1)\nscreen_Xd \u003d np.stack([screen_directions.ra.deg, screen_directions.dec.deg], axis\u003d1)\n\n# %%\n# screen_Xd is [Nd_screen, 2] ra and dec of screens in deg\n# cal_Xd is [Nd_cals,2] ra and dec of cals in deg\nfrom scipy.spatial import cKDTree\n\nkd \u003d cKDTree(cal_Xd*np.cos(cal_Xd[:,1:2]*np.pi/180.))\ndist, idx \u003d kd.query(screen_Xd*np.cos(screen_Xd[:,1:2]*np.pi/180.), k\u003d1)\nkeep_screen \u003d dist * 60 \u003e 0.5\n\nmjs_times \u003d times.mjd * 86400.\n\nconcat_Xd \u003d np.concatenate([cal_Xd, screen_Xd[keep_screen, :]], axis\u003d0)\nconcat_phase \u003d np.concatenate([cal_phase, phase_posterior[:, keep_screen, ...]], axis\u003d1)\ndist, idx \u003d kd.query(concat_Xd*np.cos(concat_Xd[:,1:2]*np.pi/180.), k\u003d1)\n\nconcat_amp \u003d cal_amp[:, idx, ...]\n\nif \u0027concat_posterior\u0027 in datapack.solsets:\n    datapack.delete_solset(\u0027concat_posterior\u0027)\n\ndatapack.add_solset(\u0027concat_posterior\u0027)\ndatapack.set_directions(None, concat_Xd * np.pi / 180.)\n\ndatapack.current_solset \u003d \u0027concat_posterior\u0027\nantenna_labels, _ \u003d datapack.antennas\npatch_names, _ \u003d datapack.directions\n\ndatapack.add_soltab(\u0027phase000\u0027, values\u003dconcat_phase,\n                    ant\u003dantenna_labels, dir\u003dpatch_names, time\u003dmjs_times, freq\u003dfreqs, pol\u003dpols)\n\ndatapack.add_soltab(\u0027amplitude000\u0027, values\u003dconcat_amp,\n                    ant\u003dantenna_labels, dir\u003dpatch_names, time\u003dmjs_times, freq\u003dfreqs, pol\u003dpols)\n"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "pycharm-149ef6aa",
      "language": "python",
      "display_name": "PyCharm (bayes_filter)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}