{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gpflow as gp\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pylab as plt\n",
    "import os\n",
    "from timeit import default_timer\n",
    "import seaborn as sns\n",
    "from bayes_filter.kernels import DTECIsotropicTimeGeneral, DTECIsotropicTimeGeneralODE\n",
    "from bayes_filter.misc import make_coord_array, safe_cholesky\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import ConvexHull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gpflow as gp\n",
    "\n",
    "from gpflow import transforms\n",
    "from gpflow import settings\n",
    "\n",
    "from gpflow.params import Parameter, Parameterized, ParamList\n",
    "from gpflow.decors import params_as_tensors, autoflow\n",
    "\n",
    "float_type = settings.float_type\n",
    "\n",
    "class GPRCustom(gp.models.GPR):\n",
    "    @gp.params_as_tensors\n",
    "    @autoflow((settings.float_type, [None, None]), (settings.float_type, [None, None]))\n",
    "    def predict_density_full_cov(self, Xnew, Ynew, ground=False):\n",
    "        \"\"\"\n",
    "        Compute the (log) density of the data Ynew at the points Xnew\n",
    "        Note that this computes the log density of the data individually,\n",
    "        ignoring correlations between them. The result is a matrix the same\n",
    "        shape as Ynew containing the log densities.\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self._build_predict(Xnew,full_cov=True)\n",
    "        #Knn + sigma^2I + Knm (Kmm + sigma^2I)^-1 Kmn\n",
    "        if ground:\n",
    "            K = pred_f_var\n",
    "            L = safe_cholesky(K[0,:,:])\n",
    "        else:\n",
    "            K = pred_f_var + self.likelihood.variance*tf.eye(tf.shape(Xnew)[0],dtype=Ynew.dtype)\n",
    "            L = tf.cholesky(K)[0,:,:]\n",
    "        return gp.logdensities.multivariate_normal(Ynew, pred_f_mean, L)\n",
    "    \n",
    "    @gp.params_as_tensors\n",
    "    @autoflow((settings.float_type, [None, None]), (settings.float_type, [None, None]))\n",
    "    def predict_density_independent(self, Xnew, Ynew, ground=False):\n",
    "        \"\"\"\n",
    "        Compute the (log) density of the data Ynew at the points Xnew\n",
    "        Note that this computes the log density of the data individually,\n",
    "        ignoring correlations between them. The result is a matrix the same\n",
    "        shape as Ynew containing the log densities.\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self._build_predict(Xnew,full_cov=False)\n",
    "        if ground:\n",
    "            var = pred_f_var\n",
    "        else:\n",
    "            #diag(Knn + sigma^2I + Knm (Kmm + sigma^2I)^-1 Kmn)\n",
    "            var = pred_f_var + self.likelihood.variance\n",
    "        return gp.logdensities.gaussian(Ynew, pred_f_mean, var)[:,0]\n",
    "    \n",
    "class DTECKernel(gp.kernels.Kernel):\n",
    "    def __init__(self, input_dim, variance=1e9**2, lengthscales=10.0,\n",
    "                 velocity=[0.,0.,0.], a = 250., b = 50., resolution=10,\n",
    "                 active_dims=None, fed_kernel='RBF', obs_type='DTEC',name=None):\n",
    "        \"\"\"\n",
    "        - input_dim is the dimension of the input to the kernel\n",
    "        - variance is the (initial) value for the variance parameter\n",
    "        - lengthscales is the initial value for the lengthscales parameter\n",
    "          defaults to 1.0 (ARD=False) or np.ones(input_dim) (ARD=True).\n",
    "        - active_dims is a list of length input_dim which controls which\n",
    "          columns of X are used.\n",
    "        \"\"\"\n",
    "        super().__init__(input_dim, active_dims, name=name)\n",
    "        self.variance = Parameter(variance, transform=transforms.positiveRescale(variance),\n",
    "                                  dtype=settings.float_type)\n",
    "        # (3,)\n",
    "        self.lengthscales = Parameter(lengthscales, transform=transforms.positiveRescale(lengthscales),\n",
    "                                      dtype=settings.float_type)\n",
    "#         # (3,)\n",
    "#         self.velocity = Parameter(velocity, transform=transforms.positive,\n",
    "#                                       dtype=settings.float_type)\n",
    "        self.a = Parameter(a, transform=transforms.positiveRescale(a),\n",
    "                                      dtype=settings.float_type)\n",
    "        self.b = Parameter(b, transform=transforms.positiveRescale(b),\n",
    "                                      dtype=settings.float_type)\n",
    "        self.resolution = resolution\n",
    "        self.obs_type = obs_type\n",
    "        self.fed_kernel = fed_kernel\n",
    "\n",
    "    @params_as_tensors\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        if not presliced:\n",
    "            X, _ = self._slice(X, None)\n",
    "        return tf.diag_part(self.K(X,None))\n",
    "\n",
    "    @params_as_tensors\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        \n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "            \n",
    "        kern = DTECIsotropicTimeGeneral(variance=self.variance, lengthscales=self.lengthscales,\n",
    "                                a= self.a, b=self.b, fed_kernel=self.fed_kernel, obs_type=self.obs_type,\n",
    "                               squeeze=True,#ode_type='adaptive',\n",
    "                               kernel_params={'resolution':self.resolution})\n",
    "        return kern.K(X,X2)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gpflow as gp\n",
    "\n",
    "from gpflow import transforms\n",
    "from gpflow import settings\n",
    "\n",
    "from gpflow.params import Parameter, Parameterized, ParamList\n",
    "from gpflow.decors import params_as_tensors, autoflow\n",
    "\n",
    "float_type = settings.float_type\n",
    "    \n",
    "class DTECKernel2(gp.kernels.Kernel):\n",
    "    def __init__(self, input_dim, variance=1.0, lengthscales=10.0,\n",
    "                 velocity=[0.,0.,0.], a = 250., b = 50., resolution=10,\n",
    "                 active_dims=None, fed_kernel='RBF', obs_type='DTEC',name=None):\n",
    "        \"\"\"\n",
    "        - input_dim is the dimension of the input to the kernel\n",
    "        - variance is the (initial) value for the variance parameter\n",
    "        - lengthscales is the initial value for the lengthscales parameter\n",
    "          defaults to 1.0 (ARD=False) or np.ones(input_dim) (ARD=True).\n",
    "        - active_dims is a list of length input_dim which controls which\n",
    "          columns of X are used.\n",
    "        \"\"\"\n",
    "        super().__init__(input_dim, active_dims, name=name)\n",
    "        self.variance = Parameter(variance, transform=transforms.positive,\n",
    "                                  dtype=settings.float_type)\n",
    "        # (3,)\n",
    "        self.lengthscales = Parameter(lengthscales*0.84, transform=transforms.positive,\n",
    "                                      dtype=settings.float_type)\n",
    "        # (3,)\n",
    "        self.velocity = Parameter(velocity, transform=transforms.positive,\n",
    "                                      dtype=settings.float_type)\n",
    "        self.a = Parameter(a, transform=transforms.positive,\n",
    "                                      dtype=settings.float_type)\n",
    "        self.b = Parameter(b, transform=transforms.positive,\n",
    "                                      dtype=settings.float_type)\n",
    "        self.resolution = resolution\n",
    "        self.obs_type = obs_type\n",
    "        self.fed_kernel = fed_kernel\n",
    "\n",
    "    @params_as_tensors\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        if not presliced:\n",
    "            X, _ = self._slice(X, None)\n",
    "        return tf.diag_part(self.K(X,None))\n",
    "\n",
    "    @params_as_tensors\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        \n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        \n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "            \n",
    "#         with tf.control_dependencies([tf.print(\"X shape\", tf.shape(X), \"X2 shape\", tf.shape(X2))]):\n",
    "        if self.fed_kernel == 'RBF':\n",
    "            iono_kern = tfp.positive_semidefinite_kernels.ExponentiatedQuadratic(amplitude=None,\n",
    "                                                                                 length_scale=self.lengthscales,\n",
    "                                                                                feature_ndims=1)\n",
    "            \n",
    "        elif self.fed_kernel == 'M52':\n",
    "            iono_kern = tfp.positive_semidefinite_kernels.MaternFiveHalves(amplitude=None,\n",
    "                                                                       length_scale=self.lengthscales,\n",
    "                                                                       feature_ndims=1)\n",
    "        elif self.fed_kernel == 'M32':\n",
    "            iono_kern = tfp.positive_semidefinite_kernels.MaternThreeHalves(amplitude=None,\n",
    "                                                                       length_scale=self.lengthscales,\n",
    "                                                                       feature_ndims=1)\n",
    "            \n",
    "        elif self.fed_kernel == 'M12':\n",
    "            iono_kern = tfp.positive_semidefinite_kernels.MaternOneHalf(amplitude=None,\n",
    "                                                                       length_scale=self.lengthscales,\n",
    "                                                                       feature_ndims=1)\n",
    "            \n",
    "        \n",
    "            \n",
    "        #N\n",
    "        times = X[:,0]\n",
    "        #N,3\n",
    "        directions = X[:,slice(1,4,1)]\n",
    "        \n",
    "        #N,3\n",
    "        antennas = X[:,slice(4,7,1)]\n",
    "        \n",
    "        #Np\n",
    "        times2 = X2[:,0]\n",
    "        #Np,3\n",
    "        directions2 = X2[:,slice(1,4,1)]\n",
    "        #Np,3\n",
    "        antennas2 = X2[:,slice(4,7,1)]\n",
    "        \n",
    "        # l1 = 0, l2 = 0\n",
    "        # assume x0 = 0,0,0\n",
    "        \n",
    "        #N\n",
    "        sec1 = tf.reciprocal(directions[:,2],name='sec1')\n",
    "        #Np\n",
    "        sec2 = tf.reciprocal(directions2[:,2],name='sec2')\n",
    "        \n",
    "        #N\n",
    "        ds1 = sec1*self.b/tf.cast(self.resolution - 1,float_type)\n",
    "        #Np\n",
    "        ds2 = sec2*self.b/tf.cast(self.resolution - 1,float_type)\n",
    "        #N,Np\n",
    "        ds1ds2 = ds1[:,None]*ds2[None,:]\n",
    "        \n",
    "        ###\n",
    "        # 1 1 terms\n",
    "        #N\n",
    "        s1m = sec1*(self.a  - (antennas[:,2] - self.velocity[2]*times)) - 0.5*sec1*self.b\n",
    "        #N\n",
    "        s1p = sec1*(self.a  - (antennas[:,2] - self.velocity[2]*times)) + 0.5*sec1*self.b\n",
    "        \n",
    "        \n",
    "        #Np\n",
    "        s2m = sec2*(self.a  - (antennas2[:,2] - self.velocity[2]*times2)) - 0.5*sec2*self.b\n",
    "        #Np\n",
    "        s2p = sec2*(self.a  - (antennas2[:,2] - self.velocity[2]*times2)) + 0.5*sec2*self.b\n",
    "        \n",
    "        #res, N\n",
    "        s1 = s1m[None,:] + ((s1p - s1m)[None,:])*tf.cast(tf.linspace(0.,1.,self.resolution)[:,None],float_type)\n",
    "#         #res, N, Np\n",
    "#         s1 = tf.tile(s1[:,:,None],tf.concat([[1],[1], tf.shape(X2)[0:1]],axis=0))\n",
    "        #res, Np\n",
    "        s2 = s2m[None,:] + ((s2p - s2m)[None,:])*tf.cast(tf.linspace(0.,1.,self.resolution)[:,None],float_type)\n",
    "#         #res, N, Np\n",
    "#         s1 = tf.tile(s1[:,None,:],tf.concat([[1], tf.shape(X)[0:1],[1]],axis=0))\n",
    "        \n",
    "        ###\n",
    "        # 0 0 terms\n",
    "        #N\n",
    "        s1m_ = sec1*(self.a + self.velocity[2]*times) - 0.5*sec1*self.b\n",
    "        #N\n",
    "        s1p_ = sec1*(self.a + self.velocity[2]*times) + 0.5*sec1*self.b\n",
    "        \n",
    "        #Np\n",
    "        s2m_ = sec2*(self.a  + self.velocity[2]*times2) - 0.5*sec2*self.b\n",
    "        #Np\n",
    "        s2p_ = sec2*(self.a  + self.velocity[2]*times2) + 0.5*sec2*self.b\n",
    "        \n",
    "        #res, N\n",
    "        s1_ = s1m_[None,:] + ((s1p_ - s1m_)[None,:])*tf.cast(tf.linspace(0.,1.,self.resolution)[:,None],float_type)\n",
    "#         #res, N, Np\n",
    "#         s1 = tf.tile(s1[:,:,None],tf.concat([[1],[1], tf.shape(X2)[0:1]],axis=0))\n",
    "        #res, Np\n",
    "        s2_ = s2m_[None,:] + ((s2p_ - s2m_)[None,:])*tf.cast(tf.linspace(0.,1.,self.resolution)[:,None],float_type)\n",
    "#         #res, N, Np\n",
    "#         s1 = tf.tile(s1[:,None,:],tf.concat([[1], tf.shape(X)[0:1],[1]],axis=0))\n",
    "        \n",
    "                       \n",
    "        # I00\n",
    "        \n",
    "        #res, N, 3\n",
    "        y1 = antennas[None,:,:] - self.velocity[None,None, :]*times[None, :, None] + directions[None, :, :]*s1[:,:,None]\n",
    "        shape1 = tf.shape(y1)\n",
    "        # res1 N, 3\n",
    "        y1 = tf.reshape(y1,(-1, 3))\n",
    "        \n",
    "        #res, Np, 3\n",
    "        y2 = antennas2[None,:,:] - self.velocity[None,None, :]*times2[None, :, None] + directions2[None, :, :]*s2[:,:,None]  \n",
    "        shape2 = tf.shape(y2)\n",
    "        # res2 Np, 3\n",
    "        y2 = tf.reshape(y2,(-1, 3))        \n",
    "        \n",
    "                                                                        \n",
    "        #res1 N, res2 Np\n",
    "        K = iono_kern.matrix(y1,y2)\n",
    "        shape = tf.concat([shape1[:2],shape2[:2]],axis=0)\n",
    "        #res1, N, res2, Np\n",
    "        K = tf.reshape(K,shape)\n",
    "        \n",
    "#         with tf.control_dependencies([tf.print(\"I00 K shape\", tf.shape(K),\"X shape\", tf.shape(X), \"X2 shape\", tf.shape(X2))]):\n",
    "        #N,Np\n",
    "        I00 = 0.25*ds1ds2 * tf.add_n([K[0,:,0,:], \n",
    "                               K[-1,:,0,:], \n",
    "                               K[0,:,-1,:], \n",
    "                               K[-1,:,-1,:],\n",
    "                               2*tf.reduce_sum(K[-1,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[0,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[:,:,-1,:],axis=[0]),\n",
    "                               2*tf.reduce_sum(K[:,:,0,:],axis=[0]),\n",
    "                               4*tf.reduce_sum(K[1:-1,:,1:-1,:],axis=[0,2])])\n",
    "        if self.obs_type == 'TEC':\n",
    "            return I00\n",
    "        \n",
    "        # I10\n",
    "        \n",
    "        #res, N, 3\n",
    "        y1 = - self.velocity[None,None, :]*times[None, :, None] + directions[None, :, :]*s1_[:,:,None]\n",
    "        shape1 = tf.shape(y1)\n",
    "        # res1 N, 3\n",
    "        y1 = tf.reshape(y1,(-1, 3))\n",
    "        \n",
    "        #res, Np, 3\n",
    "        y2 = antennas2[None,:,:] - self.velocity[None,None, :]*times2[None, :, None] + directions2[None, :, :]*s2[:,:,None]  \n",
    "        shape2 = tf.shape(y2)\n",
    "        # res2 Np, 3\n",
    "        y2 = tf.reshape(y2,(-1, 3))        \n",
    "        \n",
    "                                                                        \n",
    "        #res1 N, res2 Np\n",
    "        K = iono_kern.matrix(y1,y2)\n",
    "        shape = tf.concat([shape1[:2],shape2[:2]],axis=0)\n",
    "        #res1, N, res2, Np\n",
    "        K = tf.reshape(K,shape)\n",
    "#         with tf.control_dependencies([tf.print(\"I10 K shape\", tf.shape(K),\"X shape\", tf.shape(X), \"X2 shape\", tf.shape(X2))]):\n",
    "        #N,Np\n",
    "        I10 = 0.25*ds1ds2 * tf.add_n([K[0,:,0,:], \n",
    "                               K[-1,:,0,:], \n",
    "                               K[0,:,-1,:], \n",
    "                               K[-1,:,-1,:],\n",
    "                               2*tf.reduce_sum(K[-1,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[0,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[:,:,-1,:],axis=[0]),\n",
    "                               2*tf.reduce_sum(K[:,:,0,:],axis=[0]),\n",
    "                               4*tf.reduce_sum(K[1:-1,:,1:-1,:],axis=[0,2])])\n",
    "        # I01\n",
    "        \n",
    "        #res, N, 3\n",
    "        y1 = antennas[None,:,:] - self.velocity[None,None, :]*times[None, :, None] + directions[None, :, :]*s1[:,:,None]\n",
    "        shape1 = tf.shape(y1)\n",
    "        # res1 N, 3\n",
    "        y1 = tf.reshape(y1,(-1, 3))\n",
    "        \n",
    "        #res, Np, 3\n",
    "        y2 =  - self.velocity[None,None, :]*times2[None, :, None] + directions2[None, :, :]*s2_[:,:,None]  \n",
    "        shape2 = tf.shape(y2)\n",
    "        # res2 Np, 3\n",
    "        y2 = tf.reshape(y2,(-1, 3))        \n",
    "        \n",
    "                                                                        \n",
    "        #res1 N, res2 Np\n",
    "        K = iono_kern.matrix(y1,y2)\n",
    "        shape = tf.concat([shape1[:2],shape2[:2]],axis=0)\n",
    "        #res1, N, res2, Np\n",
    "        K = tf.reshape(K,shape)\n",
    "#         with tf.control_dependencies([tf.print(\"I01 K shape\", tf.shape(K),\"X shape\", tf.shape(X), \"X2 shape\", tf.shape(X2))]):\n",
    "        #N,Np\n",
    "        I01 = 0.25*ds1ds2 * tf.add_n([K[0,:,0,:], \n",
    "                               K[-1,:,0,:], \n",
    "                               K[0,:,-1,:], \n",
    "                               K[-1,:,-1,:],\n",
    "                               2*tf.reduce_sum(K[-1,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[0,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[:,:,-1,:],axis=[0]),\n",
    "                               2*tf.reduce_sum(K[:,:,0,:],axis=[0]),\n",
    "                               4*tf.reduce_sum(K[1:-1,:,1:-1,:],axis=[0,2])])\n",
    "        \n",
    "        # I11\n",
    "        \n",
    "        #res, N, 3\n",
    "        y1 = - self.velocity[None,None, :]*times[None, :, None] + directions[None, :, :]*s1_[:,:,None]\n",
    "        shape1 = tf.shape(y1)\n",
    "        # res1 N, 3\n",
    "        y1 = tf.reshape(y1,(-1, 3))\n",
    "        \n",
    "        #res, Np, 3\n",
    "        y2 =  - self.velocity[None,None, :]*times2[None, :, None] + directions2[None, :, :]*s2_[:,:,None]  \n",
    "        shape2 = tf.shape(y2)\n",
    "        # res2 Np, 3\n",
    "        y2 = tf.reshape(y2,(-1, 3))        \n",
    "        \n",
    "                                                                        \n",
    "        #res1 N, res2 Np\n",
    "        K = iono_kern.matrix(y1,y2)\n",
    "        shape = tf.concat([shape1[:2],shape2[:2]],axis=0)\n",
    "        #res1, N, res2, Np\n",
    "        K = tf.reshape(K,shape)\n",
    "#         with tf.control_dependencies([tf.print(\"I11 K shape\", tf.shape(K),\"X shape\", tf.shape(X), \"X2 shape\", tf.shape(X2),'ds1ds2',tf.shape(ds1ds2))]):\n",
    "        #N,Np\n",
    "        I11 = 0.25*ds1ds2 * tf.add_n([K[0,:,0,:], \n",
    "                               K[-1,:,0,:], \n",
    "                               K[0,:,-1,:], \n",
    "                               K[-1,:,-1,:],\n",
    "                               2*tf.reduce_sum(K[-1,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[0,:,:,:],axis=[1]),\n",
    "                               2*tf.reduce_sum(K[:,:,-1,:],axis=[0]),\n",
    "                               2*tf.reduce_sum(K[:,:,0,:],axis=[0]),\n",
    "                               4*tf.reduce_sum(K[1:-1,:,1:-1,:],axis=[0,2])])\n",
    "        \n",
    "        \n",
    "        return self.variance*(I00 + I11 - I01 - I10)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(paramfile, param, strip_unit=True, makefloat=True):\n",
    "    with open(paramfile,'r') as f:\n",
    "        res = None\n",
    "        for l in f.readlines():\n",
    "            if l.strip().startswith(param):\n",
    "                p = l.split(':')[1]\n",
    "                p = p.strip()\n",
    "                if strip_unit:\n",
    "                    res = (p.split(\" \")[0]).strip()\n",
    "                else:\n",
    "                    res = p\n",
    "        if res is None:\n",
    "            raise ValueError(\"could not find {}\".format(param))\n",
    "        return float(res) if makefloat else res\n",
    "\n",
    "get_param('/home/albert/Dropbox/selective_sync/Fluxus_Glacialis/data/set23/simParameters.txt','standardDeviationFED')\n",
    "\n",
    "def min_dist_lines(x0,k0,x1,k1):\n",
    "    #N\n",
    "    a = np.einsum('na,na->n',k0,k0)\n",
    "    #N\n",
    "    b = np.einsum('na,na->n',k0,k1)\n",
    "    #M,M\n",
    "    c = np.einsum('na,na->n',k1,k1)\n",
    "    #N,3\n",
    "    w0 = x0 - x1\n",
    "    #N\n",
    "    d = np.einsum('na,na->n',k0,w0)\n",
    "    #N\n",
    "    e = np.einsum('na,na->n',k1,w0)\n",
    "    acb2 = a*c - b**2\n",
    "    sc = (b*e - c*d)/acb2\n",
    "    tc = (a*e - b*d)/acb2\n",
    "    dist = np.linalg.norm(x0 - x1 + sc[:,None] * k0 - tc[:,None] * k1,axis=1)\n",
    "    return np.where(np.abs(acb2)>0., dist, e/c)\n",
    "\n",
    "def batch_min_dist_lines(x0,k0,x1,k1):\n",
    "    N = x0.shape[0]\n",
    "    M = x1.shape[0]\n",
    "    x0 = np.tile(x0[:,None,:],(1,M,1)).reshape((-1,3))\n",
    "    k0 = np.tile(k0[:,None,:],(1,M,1)).reshape((-1,3))\n",
    "    x1 = np.tile(x1[None,:,:],(N,1,1)).reshape((-1,3))\n",
    "    k1 = np.tile(k1[None,:,:],(N,1,1)).reshape((-1,3))\n",
    "    return min_dist_lines(x0,k0,x1,k1).reshape((N,M))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_and_coords(data_dir= './dtec_gen_data/set14',\n",
    "                        dtec_sigma=0.001, test_split = 0.3, seed=0,split_dirs=False, no_split=False, cut=0.5, scale=1.):\n",
    "    np.random.seed(seed)\n",
    "    #Na, Nd, Nt\n",
    "    dtecs = np.load(os.path.join(data_dir,'simDTECs.npy'))/0.01\n",
    "    Na, Nd, Nt = dtecs.shape\n",
    "    out_shape = (Na,Nd,Nt)\n",
    "    #Nt, Nd, Na\n",
    "    dtecs = dtecs.transpose((2,1,0))\n",
    "    if scale is 'std':\n",
    "        scale = dtecs.std()\n",
    "    \n",
    "    dtecs /= scale\n",
    "    \n",
    "    dtec_sigma = dtec_sigma/scale\n",
    "\n",
    "    dtecs += dtec_sigma*np.random.normal(size=dtecs.shape)\n",
    "\n",
    "    #Na, 3\n",
    "    antennas = np.stack([np.load(os.path.join(data_dir,'simAntennaXs.npy')),\n",
    "                         np.load(os.path.join(data_dir,'simAntennaYs.npy')),\n",
    "                         np.load(os.path.join(data_dir,'simAntennaZs.npy'))],axis=1)\n",
    "    \n",
    "    # select antennas < 500m from each other\n",
    "    print(\"Limiting to ant spacing [km]:\",cut)\n",
    "    antennas_sel = []\n",
    "    for i in range(Na):\n",
    "        if len(antennas_sel) == 0:\n",
    "            antennas_sel.append(i)\n",
    "            continue\n",
    "        already = antennas[antennas_sel,:]\n",
    "        dist = np.linalg.norm(already - antennas[i,:],axis=1)\n",
    "        if np.any(dist < cut):\n",
    "            continue\n",
    "        antennas_sel.append(i)\n",
    "    antennas = antennas[antennas_sel,:]\n",
    "    dtecs = dtecs[:,:,antennas_sel]\n",
    "    print('Selecting {} antennas'.format(len(antennas_sel)),antennas_sel)\n",
    "\n",
    "    if not os.path.exists(os.path.join(data_dir,'simKXs.npy')):\n",
    "        #Nd, 2\n",
    "        directions = np.stack([np.load(os.path.join(data_dir,'simAnglesX.npy')),\n",
    "                             np.load(os.path.join(data_dir,'simAnglesZ.npy'))],axis=1)\n",
    "        directions *= np.pi/180.\n",
    "        directions = np.angle(np.exp(1j*directions))\n",
    "        kx = np.cos(directions[:,0])*np.sin(directions[:,1])\n",
    "        ky = np.sin(directions[:,0])*np.sin(directions[:,1])\n",
    "        kz = np.sqrt(1.-kx**2 - ky**2)\n",
    "        #Nd,3\n",
    "        directions = np.stack([kx,ky,kz],axis=1)\n",
    "    else:\n",
    "        #Nd, 3\n",
    "        directions = np.stack([np.load(os.path.join(data_dir,'simKXs.npy')), \n",
    "                          np.load(os.path.join(data_dir,'simKYs.npy')),\n",
    "                          np.load(os.path.join(data_dir,'simKZs.npy'))],axis=1)\n",
    "\n",
    "    #Nt, 1\n",
    "    times = np.load(os.path.join(data_dir,'simTimes.npy'))[:,None]\n",
    "#     times = times[:1,:]\n",
    "\n",
    "    X = make_coord_array(times, directions, antennas)\n",
    "    X0 = np.copy(X[:,4:7])\n",
    "    X0[:,:] = 0.\n",
    "    X = np.concatenate([X,X0],axis=1)\n",
    "    Y = dtecs.reshape((-1,1))\n",
    "\n",
    "    if split_dirs:\n",
    "        Nd = directions.shape[0]\n",
    "        if test_split > 1:\n",
    "            test_split = 1. - test_split / float(Nd)\n",
    "        test_Nd = int(test_split*Nd)\n",
    "        train_Nd = Nd - test_Nd\n",
    "        \n",
    "        split = np.random.choice(Nd,size=Nd,replace=False)\n",
    "        train_idx = split[:train_Nd]\n",
    "        test_idx = split[train_Nd:]\n",
    "\n",
    "        X_train = make_coord_array(times, directions[train_idx,:], antennas)\n",
    "        X0 = np.copy(X_train[:,4:7])\n",
    "        X0[:,:] = 0.\n",
    "        X_train = np.concatenate([X_train,X0],axis=1)\n",
    "        Y_train = dtecs[:,train_idx,:].reshape((-1,1))\n",
    "\n",
    "        X_test = make_coord_array(times, directions[test_idx,:], antennas)\n",
    "        X0 = np.copy(X_test[:,4:7])\n",
    "        X0[:,:] = 0.\n",
    "        X_test = np.concatenate([X_test,X0],axis=1)\n",
    "        Y_test = dtecs[:,test_idx,:].reshape((-1,1))\n",
    "        \n",
    "#         ch = ConvexHull(np.concatenate([np.zeros((1,3)),X_train[:,1:4]],axis=0))\n",
    "        ch = ConvexHull(X_train[:,1:4])\n",
    "        area = ch.area/2.\n",
    "        \n",
    "        cone_density = train_Nd/area\n",
    "\n",
    "        return (X_train, Y_train), (X_test, Y_test), cone_density, (X, Y), scale\n",
    "    \n",
    "    #Nt*Nd*Na, ndims\n",
    "    X = make_coord_array(times, directions, antennas)\n",
    "    X0 = np.copy(X[:,4:7])\n",
    "    X0[:,:] = 0.\n",
    "    X = np.concatenate([X,X0],axis=1)\n",
    "    Y = dtecs.reshape((-1,1))\n",
    "    \n",
    "    L = X.shape[0]\n",
    "    test_L = int(test_split*L)\n",
    "    train_L = L - test_L\n",
    "    np.random.seed(seed)\n",
    "    split = np.random.choice(L,size=L,replace=False)\n",
    "\n",
    "    train_idx = split[:train_L]\n",
    "    test_idx = split[train_L:]\n",
    "\n",
    "    X_train = X[train_idx, :]\n",
    "    Y_train = Y[train_idx, :]\n",
    "\n",
    "    X_test = X[test_idx, :]\n",
    "    Y_test = Y[test_idx, :]\n",
    "    \n",
    "    directions = X_train[:,1:4]\n",
    "    origins = X_train[:,4:7]\n",
    "    \n",
    "    ch = ConvexHull(np.concatenate([np.zeros((1,3)),X_train[:,1:4]],axis=0))\n",
    "    volume = ch.volume\n",
    "    cone_density = train_L/volume\n",
    "    \n",
    "    return (X_train, Y_train), (X_test, Y_test), cone_density, (X, Y)\n",
    "\n",
    "def result_B(data_dir = './dtec_gen_data/set11',\n",
    "            dtec_sigma=0.001, res=4, test_split = 0.3, seed=0, \n",
    "            fed_kernel='RBF'):\n",
    "    \n",
    "    (X_train, Y_train), (X_test, Y_test) = get_data_and_coords(data_dir,\n",
    "                        dtec_sigma, test_split, seed)\n",
    "    train_L = X_train.shape[0]\n",
    "    test_L = X_test.shape[0]\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        kern = DTECKernel(10,\n",
    "                        variance=get_param(paramfile, 'standardDeviationFED')**2,\n",
    "                        lengthscales=np.random.uniform(5.,30.),\n",
    "                        a = np.random.uniform(100.,500.), \n",
    "                        b = np.random.uniform(40.,150.), \n",
    "                        resolution=res, \n",
    "                        fed_kernel=fed_kernel)\n",
    "        \n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.variance.trainable = False\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(prior): {}\".format(logp_prior))\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        kern_logp =  m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(post): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(post) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "#         print(m.as_pandas_table().style)\n",
    "        if fed_kernel=='RBF':\n",
    "            fwhm = kern.lengthscales.value / 0.84\n",
    "        elif fed_kernel == 'M12':\n",
    "            fwhm = kern.lengthscales.value / 1.44\n",
    "        print(\"FWHM: {}\".format(fwhm))\n",
    "        \n",
    "        return logp_prior,kern_logp,kern_logp_test,opt_params\n",
    "    \n",
    "def result_A(data_dir = './dtec_gen_data/set18',\n",
    "            dtec_sigma=0.001, res=4, test_split = 0.3, seed=0, \n",
    "            fed_kernel='RBF'):\n",
    "    \n",
    "    paramfile = os.path.join(data_dir,'simParameters.txt')\n",
    "    dtec_sigma = dtec_sigma*get_param(paramfile, 'standardDeviationFED')*get_param(paramfile, 'ionosphereThickness')*1000./1e16\n",
    "    \n",
    "    (X_train, Y_train), (X_test, Y_test), cone_density, (X, Y) =  get_data_and_coords(data_dir,\n",
    "                        dtec_sigma, test_split, seed, split_dirs=True)\n",
    "    train_L = X_train.shape[0]\n",
    "    test_L = X_test.shape[0]\n",
    "    print(\"Train size: {} Test size: {}\".format(train_L, test_L))  \n",
    "    print(\"Cone_density: {}\".format(cone_density)) \n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        kern = DTECKernel(10,\n",
    "                        variance=get_param(paramfile, 'standardDeviationFED')**2,\n",
    "                        lengthscales=get_param(paramfile, 'scaleSpatialCorrelation12')*0.84,\n",
    "                        a = get_param(paramfile, 'ionosphereHeight'), \n",
    "                        b = get_param(paramfile, 'ionosphereThickness'), \n",
    "                        resolution=res, \n",
    "                        fed_kernel=fed_kernel)\n",
    "        \n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.variance.trainable = False\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "#         print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "#         t0 = default_timer()\n",
    "# #         gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "#         t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "#         print(m.as_pandas_table().style)\n",
    "        if fed_kernel=='RBF':\n",
    "            fwhm = kern.lengthscales.value / 0.84\n",
    "        elif fed_kernel == 'M12':\n",
    "            fwhm = kern.lengthscales.value / 1.44\n",
    "        print(\"FWHM: {}\".format(fwhm))\n",
    "        \n",
    "        return cone_density,logp_prior,kern_logp,kern_logp_test,opt_params\n",
    "    \n",
    "    \n",
    "def resultA(data_dir = './dtec_gen_data/set21',\n",
    "            dtec_sigma=0.001, res=4, test_split = 0.3, seed=0, \n",
    "            fed_kernel='RBF'):\n",
    "    \n",
    "    paramfile = os.path.join(data_dir,'simParameters.txt')\n",
    "    sigma = dtec_sigma*get_param(paramfile, 'standardDeviationFED')*get_param(paramfile, 'ionosphereThickness')*1000./1e16\n",
    "        \n",
    "    (X_train, Y_train), (X_test, Y_test), cone_density, (X,Y), y_std = get_data_and_coords(data_dir,\n",
    "                        sigma, test_split, seed, split_dirs=True, scale='std')\n",
    "    \n",
    "    dtec_sigma /= y_std\n",
    "    \n",
    "    train_L = X_train.shape[0]\n",
    "    test_L = X_test.shape[0]\n",
    "    print(\"Train size: {} Test size: {}\".format(train_L, test_L))  \n",
    "    print(\"Sigma: {}\".format(dtec_sigma))\n",
    "    #N,M\n",
    "    angle = (180./np.pi)*np.arccos(np.einsum(\"ns,ms->nm\",X_test[:,1:4],X_train[:,1:4]))\n",
    "    #N\n",
    "    nearest = np.min(angle,axis=1)\n",
    "    \n",
    "    logp_data = np.zeros(6)\n",
    "    logp_data_given_data = np.zeros(6)\n",
    "    logp_test_ind = np.zeros([6,nearest.shape[0]])\n",
    "    logp_test = np.zeros(6)\n",
    "    kern_var = np.zeros(4)\n",
    "    kern_dirls = np.zeros(4)\n",
    "    kern_antls = np.zeros(4)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"Matern12 results\")\n",
    "    #     kern = gp.kernels.RBF(6,ARD=True,\n",
    "    #                           lengthscales=[1., np.std(directions), np.std(directions),10.,10.,10.],variance = np.var(dtecs))\n",
    "        kern_time = gp.kernels.Matern12(1,active_dims=slice(0,1,1),lengthscales=10.,variance = np.var(Y_train))\n",
    "        kern_dir = gp.kernels.Matern12(2,active_dims=slice(1,3,1),lengthscales=np.std(X_train[:,1:3]),variance = 1.)\n",
    "        kern_dir.variance.trainable = False\n",
    "        kern_ant = gp.kernels.Matern12(3,active_dims=slice(4,7,1),lengthscales=np.std(X_train[:,4:7]),variance = 1.)\n",
    "        kern_ant.variance.trainable = False\n",
    "        kern = kern_time * kern_dir * kern_ant\n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.trainable = False\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        ystar, varstar = m.predict_y(X)\n",
    "        pred_res_m12 = {'X':X, 'Y':Y*y_std, \n",
    "                        'X_train':X_train, 'Y_train':Y_train*y_std, \n",
    "                        'ystar':ystar*y_std, 'stdstar':np.sqrt(varstar)*y_std}\n",
    "        \n",
    "        logp_data[0] = logp_prior\n",
    "        logp_test[0] = kern_logp_test\n",
    "        logp_data_given_data[0] = kern_logp\n",
    "        logp_test_ind[0,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        kern_var[0] = opt_params['GPRCustom/kern/kernels/0/variance']\n",
    "        kern_dirls[0] = opt_params['GPRCustom/kern/kernels/1/lengthscales']\n",
    "        kern_antls[0] = opt_params['GPRCustom/kern/kernels/2/lengthscales']  \n",
    "     \n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"Matern32 results\")\n",
    "    #     kern = gp.kernels.RBF(6,ARD=True,\n",
    "    #                           lengthscales=[1., np.std(directions), np.std(directions),10.,10.,10.],variance = np.var(dtecs))\n",
    "        kern_time = gp.kernels.Matern32(1,active_dims=slice(0,1,1),lengthscales=10.,variance = np.var(Y_train))\n",
    "        kern_dir = gp.kernels.Matern32(2,active_dims=slice(1,3,1),lengthscales=np.std(X_train[:,1:3]),variance = 1.)\n",
    "        kern_dir.variance.trainable = False\n",
    "        kern_ant = gp.kernels.Matern32(3,active_dims=slice(4,7,1),lengthscales=np.std(X_train[:,4:7]),variance = 1.)\n",
    "        kern_ant.variance.trainable = False\n",
    "        kern = kern_time * kern_dir * kern_ant\n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.trainable = False\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        ystar, varstar = m.predict_y(X)\n",
    "        pred_res_m32 = {'X':X, 'Y':Y*y_std, \n",
    "                        'X_train':X_train, 'Y_train':Y_train*y_std, \n",
    "                        'ystar':ystar*y_std, 'stdstar':np.sqrt(varstar)*y_std}\n",
    "        \n",
    "        logp_data[1] = logp_prior\n",
    "        logp_test[1] = kern_logp_test\n",
    "        logp_data_given_data[1] = kern_logp\n",
    "        logp_test_ind[1,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        kern_var[1] = opt_params['GPRCustom/kern/kernels/0/variance']\n",
    "        kern_dirls[1] = opt_params['GPRCustom/kern/kernels/1/lengthscales']\n",
    "        kern_antls[1] = opt_params['GPRCustom/kern/kernels/2/lengthscales']   \n",
    "    \n",
    "     \n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"Matern52 results\")\n",
    "    #     kern = gp.kernels.RBF(6,ARD=True,\n",
    "    #                           lengthscales=[1., np.std(directions), np.std(directions),10.,10.,10.],variance = np.var(dtecs))\n",
    "        kern_time = gp.kernels.Matern52(1,active_dims=slice(0,1,1),lengthscales=10.,variance = np.var(Y_train))\n",
    "        kern_dir = gp.kernels.Matern52(2,active_dims=slice(1,3,1),lengthscales=np.std(X_train[:,1:3]),variance = 1.)\n",
    "        kern_dir.variance.trainable = False\n",
    "        kern_ant = gp.kernels.Matern52(3,active_dims=slice(4,7,1),lengthscales=np.std(X_train[:,4:7]),variance = 1.)\n",
    "        kern_ant.variance.trainable = False\n",
    "        kern = kern_time * kern_dir * kern_ant\n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.trainable = False\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        ystar, varstar = m.predict_y(X)\n",
    "        pred_res_m52 = {'X':X, 'Y':Y*y_std, \n",
    "                        'X_train':X_train, 'Y_train':Y_train*y_std, \n",
    "                        'ystar':ystar*y_std, 'stdstar':np.sqrt(varstar)*y_std}\n",
    "        \n",
    "        logp_data[2] = logp_prior\n",
    "        logp_test[2] = kern_logp_test\n",
    "        logp_data_given_data[2] = kern_logp\n",
    "        logp_test_ind[2,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        kern_var[2] = opt_params['GPRCustom/kern/kernels/0/variance']\n",
    "        kern_dirls[2] = opt_params['GPRCustom/kern/kernels/1/lengthscales']\n",
    "        kern_antls[2] = opt_params['GPRCustom/kern/kernels/2/lengthscales']    \n",
    "        \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"RBF results\")\n",
    "    #     kern = gp.kernels.RBF(6,ARD=True,\n",
    "    #                           lengthscales=[1., np.std(directions), np.std(directions),10.,10.,10.],variance = np.var(dtecs))\n",
    "        kern_time = gp.kernels.RBF(1,active_dims=slice(0,1,1),lengthscales=10.,variance = np.var(Y_train))\n",
    "        kern_dir = gp.kernels.RBF(2,active_dims=slice(1,3,1),lengthscales=np.std(X_train[:,1:3]),variance = 1.)\n",
    "        kern_dir.variance.trainable = False\n",
    "        kern_ant = gp.kernels.RBF(3,active_dims=slice(4,7,1),lengthscales=np.std(X_train[:,4:7]),variance = 1.)\n",
    "        kern_ant.variance.trainable = False\n",
    "        kern = kern_time * kern_dir * kern_ant\n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.trainable = False\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        \n",
    "        ystar, varstar = m.predict_y(X)\n",
    "        pred_res_rbf = {'X':X, 'Y':Y*y_std, \n",
    "                        'X_train':X_train, 'Y_train':Y_train*y_std, \n",
    "                        'ystar':ystar*y_std, 'stdstar':np.sqrt(varstar)*y_std}\n",
    "        \n",
    "        logp_data[3] = logp_prior\n",
    "        logp_test[3] = kern_logp_test\n",
    "        logp_data_given_data[3] = kern_logp\n",
    "        logp_test_ind[3,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        kern_var[3] = opt_params['GPRCustom/kern/kernels/0/variance']\n",
    "        kern_dirls[3] = opt_params['GPRCustom/kern/kernels/1/lengthscales']\n",
    "        kern_antls[3] = opt_params['GPRCustom/kern/kernels/2/lengthscales']  \n",
    "        \n",
    "    if fed_kernel == 'M32':\n",
    "        f = 0.969\n",
    "    if fed_kernel == 'RBF':\n",
    "        f = 1.177\n",
    "        \n",
    "#     with tf.Session(graph=tf.Graph()) as sess:\n",
    "#         print(\"DTEC kernel results\")\n",
    "#         kern = DTECKernel(10,\n",
    "#                         variance=(get_param(paramfile, 'standardDeviationFED')/y_std)**2,\n",
    "#                         lengthscales=get_param(paramfile, 'scaleSpatialCorrelation12')/f,\n",
    "#                         a = get_param(paramfile, 'ionosphereHeight'), \n",
    "#                         b = get_param(paramfile, 'ionosphereThickness'), \n",
    "#                         resolution=res, \n",
    "#                         fed_kernel=fed_kernel)\n",
    "        \n",
    "#         m = GPRCustom(X_train,Y_train,kern)\n",
    "#         m.likelihood.variance = dtec_sigma**2\n",
    "#         m.likelihood.variance.trainable = False\n",
    "#         logp_prior =  m.compute_log_likelihood()/train_L\n",
    "# #         print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "# #         t0 = default_timer()\n",
    "# # #         gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "# #         t = default_timer() - t0\n",
    "#         logp_prior =  m.compute_log_likelihood()/train_L\n",
    "#         print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "#         kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "#         kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "#         print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "#         print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "#         print('getting parameters')\n",
    "#         opt_params = m.read_trainables()\n",
    "# # #         print(m.as_pandas_table().style)\n",
    "# #         if fed_kernel=='RBF':\n",
    "# #             fwhm = kern.lengthscales.value / 0.84\n",
    "# #         elif fed_kernel == 'M12':\n",
    "# #             fwhm = kern.lengthscales.value / 1.44\n",
    "# #         print(\"FWHM: {}\".format(fwhm))\n",
    "        \n",
    "#         logp_data[4] = logp_prior\n",
    "#         logp_test[4] = kern_logp_test\n",
    "#         logp_data_given_data[4] = kern_logp\n",
    "#         logp_test_ind[4,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"DTEC kernel results (learning)\")\n",
    "        kern = DTECKernel(10,\n",
    "                        variance=np.random.normal(1.,0.25),#*(get_param(paramfile, 'standardDeviationFED')/y_std)**2,\n",
    "                        lengthscales=np.random.normal(1.,0.25)*get_param(paramfile, 'scaleSpatialCorrelation12')/f,\n",
    "                        a = np.random.normal(1.,0.25)*get_param(paramfile, 'ionosphereHeight'), \n",
    "                        b = np.random.normal(1.,0.25)*get_param(paramfile, 'ionosphereThickness'), \n",
    "                        resolution=8, \n",
    "                        fed_kernel=fed_kernel)\n",
    "        \n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.variance.trainable = False\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "        kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "        print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "        print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        \n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        ystar, varstar = m.predict_y(X)\n",
    "        pred_res_dtec = {'X':X, 'Y':Y*y_std, \n",
    "                        'X_train':X_train, 'Y_train':Y_train*y_std, \n",
    "                        'ystar':ystar*y_std, 'stdstar':np.sqrt(varstar)*y_std}\n",
    "        \n",
    "        \n",
    "        logp_data[5] = logp_prior\n",
    "        logp_test[5] = kern_logp_test\n",
    "        logp_data_given_data[5] = kern_logp\n",
    "        logp_test_ind[5,:] = m.predict_density_independent(X_test,Y_test).reshape((-1,))\n",
    "        learned_hp = np.array([kern.variance.value*y_std**2, kern.lengthscales.value, kern.a.value, kern.b.value])\n",
    "        print(\"Learned HP: {}\".format(learned_hp))\n",
    "        \n",
    "    return dtec_sigma*y_std, nearest, logp_data, logp_test, logp_data_given_data, logp_test_ind,kern_var,kern_dirls,kern_antls,pred_res_m12,pred_res_m32, pred_res_m52, pred_res_rbf, pred_res_dtec, learned_hp\n",
    "\n",
    "\n",
    "def test_hp_learn(data_dir = './dtec_gen_data/set21',\n",
    "            dtec_sigma=0.001, res=4, test_split = 0.3, seed=0, \n",
    "            fed_kernel='RBF'):\n",
    "    \n",
    "    paramfile = os.path.join(data_dir,'simParameters.txt')\n",
    "    sigma = dtec_sigma*get_param(paramfile, 'standardDeviationFED')*get_param(paramfile, 'ionosphereThickness')*1000./1e16\n",
    "    \n",
    "    \n",
    "    (X_train, Y_train), (X_test, Y_test), cone_density, (X,Y), y_std = get_data_and_coords(data_dir,\n",
    "                        sigma, test_split, seed, split_dirs=True,cut=5., scale='std')\n",
    "    \n",
    "    dtec_sigma /= y_std\n",
    "    \n",
    "    train_L = X_train.shape[0]\n",
    "    test_L = X_test.shape[0]\n",
    "    print(\"Train size: {} Test size: {}\".format(train_L, test_L))  \n",
    "    print(\"Stddev(Y): {}, Sigma: {}\".format(y_std, dtec_sigma))\n",
    "    #N,M\n",
    "    angle = (180./np.pi)*np.arccos(np.einsum(\"ns,ms->nm\",X_test[:,1:4],X_train[:,1:4]))\n",
    "    #N\n",
    "    nearest = np.min(angle,axis=1)\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print(\"DTEC kernel results (learning)\")\n",
    "        kern = DTECKernel(10,\n",
    "                        variance=np.random.normal(1.,0.25),#*(get_param(paramfile, 'standardDeviationFED')/y_std)**2,\n",
    "                        lengthscales=np.random.normal(1.,0.25)*get_param(paramfile, 'scaleSpatialCorrelation12')*0.84,\n",
    "                        a = np.random.normal(1.,0.25)*get_param(paramfile, 'ionosphereHeight'), \n",
    "                        b = np.random.normal(1.,0.25)*get_param(paramfile, 'ionosphereThickness'), \n",
    "                        resolution=8, \n",
    "                        fed_kernel=fed_kernel)\n",
    "        \n",
    "#         kern = DTECKernel2(7,\n",
    "#                         variance=np.random.normal(1.,0.001),\n",
    "#                         lengthscales=np.random.normal(1.,0.001)*get_param(paramfile, 'scaleSpatialCorrelation12')*0.84,\n",
    "#                         a = np.random.normal(1.,0.001)*get_param(paramfile, 'ionosphereHeight'), \n",
    "#                         b = np.random.normal(1.,0.001)*get_param(paramfile, 'ionosphereThickness'), \n",
    "#                         resolution=8, \n",
    "#                         fed_kernel=fed_kernel)\n",
    "        print(kern)\n",
    "        \n",
    "        m = GPRCustom(X_train,Y_train,kern)\n",
    "        m.likelihood.variance = dtec_sigma**2\n",
    "        m.likelihood.variance.trainable = False\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "        t0 = default_timer()\n",
    "        gp.train.ScipyOptimizer().minimize(m,maxiter=100)\n",
    "        t = default_timer() - t0\n",
    "        logp_prior =  m.compute_log_likelihood()/train_L\n",
    "        print(\"Kern logP(Y): {}\".format(logp_prior))\n",
    "#         kern_logp =  m.predict_density_full_cov(X_train,Y_train).sum()/train_L#m.compute_log_likelihood()/train_L\n",
    "#         kern_logp_test = m.predict_density_full_cov(X_test,Y_test).sum()/test_L\n",
    "#         print(\"Kern logP(Y | Y): {}\".format(kern_logp))\n",
    "#         print(\"Kern logP(Y* | Y) test: {}\".format(kern_logp_test))\n",
    "        \n",
    "        print('getting parameters')\n",
    "        opt_params = m.read_trainables()\n",
    "        \n",
    "        learned_hp = np.array([kern.variance.value*y_std**2, kern.lengthscales.value, kern.a.value, kern.b.value])\n",
    "        print(\"Learned HP: {}\".format(learned_hp))\n",
    "        print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting to ant spacing [km]: 5.0\n",
      "Selecting 13 antennas [0, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 22, 23]\n",
      "Train size: 195 Test size: 780\n",
      "Stddev(Y): 1.225619141373993, Sigma: 0.0008159141500343571\n",
      "DTEC kernel results (learning)\n",
      "                             class prior                transform  trainable  \\\n",
      "DTECKernel/a             Parameter  None  164.70261061984797* +ve       True   \n",
      "DTECKernel/b             Parameter  None     99.453252280273* +ve       True   \n",
      "DTECKernel/lengthscales  Parameter  None  15.718882359219458* +ve       True   \n",
      "DTECKernel/variance      Parameter  None  0.7902176492894466* +ve       True   \n",
      "\n",
      "                        shape  fixed_shape               value  \n",
      "DTECKernel/a               ()         True  164.70261061984797  \n",
      "DTECKernel/b               ()         True     99.453252280273  \n",
      "DTECKernel/lengthscales    ()         True  15.718882359219458  \n",
      "DTECKernel/variance        ()         True  0.7902176492894466  \n",
      "Kern logP(Y): -1.5116971037611544\n",
      "Kern logP(Y): 0.8141807553261236\n",
      "getting parameters\n",
      "Learned HP: [6.42421662e-03 1.81466750e+01 2.41285229e+02 5.89993716e+01]\n",
      "                                   class prior                transform  \\\n",
      "GPRCustom/kern/a               Parameter  None  164.70261061984797* +ve   \n",
      "GPRCustom/kern/b               Parameter  None     99.453252280273* +ve   \n",
      "GPRCustom/kern/lengthscales    Parameter  None  15.718882359219458* +ve   \n",
      "GPRCustom/kern/variance        Parameter  None  0.7902176492894466* +ve   \n",
      "GPRCustom/likelihood/variance  Parameter  None                      +ve   \n",
      "\n",
      "                               trainable shape  fixed_shape  \\\n",
      "GPRCustom/kern/a                    True    ()         True   \n",
      "GPRCustom/kern/b                    True    ()         True   \n",
      "GPRCustom/kern/lengthscales         True    ()         True   \n",
      "GPRCustom/kern/variance             True    ()         True   \n",
      "GPRCustom/likelihood/variance      False    ()         True   \n",
      "\n",
      "                                               value  \n",
      "GPRCustom/kern/a                   241.2852288991637  \n",
      "GPRCustom/kern/b                     58.999371610853  \n",
      "GPRCustom/kern/lengthscales       18.146675010681538  \n",
      "GPRCustom/kern/variance         0.004276703151322668  \n",
      "GPRCustom/likelihood/variance  6.657159002262873e-07  \n"
     ]
    }
   ],
   "source": [
    "test_hp_learn(data_dir = '/home/albert/Dropbox/selective_sync/Fluxus_Glacialis/data/set23',\n",
    "            dtec_sigma=0.001, res=8, test_split = 15, seed=0, \n",
    "            fed_kernel='M32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "noise = []\n",
    "num_directions = []\n",
    "\n",
    "for n in np.arange(11):\n",
    "    sigma = 0.001*2**n\n",
    "    for test_split in [10,20,30,40,50]:\n",
    "        res_ = resultA(data_dir = '/home/albert/Dropbox/selective_sync/Fluxus_Glacialis/data/set23',\n",
    "            dtec_sigma=sigma, res=8, test_split = test_split, seed=test_split, \n",
    "            fed_kernel='M32')\n",
    "        noise.append(res_[0])\n",
    "        results.append(res_[1:])\n",
    "        num_directions.append(test_split)\n",
    "        \n",
    "np.savez('set23_res_rescale_norm_test.npz',results=results,noise=noise,num_directions=num_directions)\n",
    "\n",
    "results = []\n",
    "noise = []\n",
    "num_directions = []\n",
    "\n",
    "for n in np.arange(11):\n",
    "    sigma = 0.001*2**n\n",
    "    for test_split in [10,20,30,40,50]:\n",
    "        res_ = resultA(data_dir = '/home/albert/Dropbox/selective_sync/Fluxus_Glacialis/data/set24',\n",
    "            dtec_sigma=sigma, res=8, test_split = test_split, seed=test_split, \n",
    "            fed_kernel='RBF')\n",
    "        noise.append(res_[0])\n",
    "        results.append(res_[1:])\n",
    "        num_directions.append(test_split)\n",
    "        \n",
    "np.savez('set24_res_rescale_norm_test.npz',results=results,noise=noise,num_directions=num_directions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "\n",
    "data = np.load('set23_res_rescale_norm.npz')\n",
    "results = data['results']\n",
    "noise23 = data['noise']\n",
    "num_directions23 = data['num_directions']\n",
    "\n",
    "logp_data23 = np.zeros([11,5,6])\n",
    "logp_test23 = np.zeros([11,5,6])\n",
    "learned_hp23 = np.zeros([11,5,4])\n",
    "\n",
    "logp_test_ind23 = []\n",
    "nearest23 = []\n",
    "\n",
    "c = 0\n",
    "for n in np.arange(11):\n",
    "    for i,test_split in enumerate([10,20,30,40,50]):\n",
    "        logp_data23[n,i,:] = results[c][1]\n",
    "        logp_test23[n,i,:] = results[c][2]\n",
    "        learned_hp23[n,i,:] = results[c][-1]\n",
    "        a = np.argsort(results[c][0])\n",
    "        nearest23.append(results[c][0][a])\n",
    "        logp_test_ind23.append(medfilt(results[c][4][:,a],1))\n",
    "        c += 1\n",
    "        \n",
    "noises23 = np.unique(noise23)\n",
    "num_dirs23 = np.unique(num_directions23)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = np.load('set24_res_rescale_norm.npz')\n",
    "results = data['results']\n",
    "noise24 = data['noise']\n",
    "num_directions24 = data['num_directions']\n",
    "\n",
    "logp_data24 = np.zeros([11,5,6])\n",
    "logp_test24 = np.zeros([11,5,6])\n",
    "learned_hp24 = np.zeros([11,5,4])\n",
    "\n",
    "logp_test_ind24 = []\n",
    "nearest24 = []\n",
    "\n",
    "c = 0\n",
    "for n in np.arange(11):\n",
    "    for i,test_split in enumerate([10,20,30,40,50]):\n",
    "        logp_data24[n,i,:] = results[c][1]\n",
    "        logp_test24[n,i,:] = results[c][2]\n",
    "        learned_hp24[n,i,:] = results[c][-1]\n",
    "        a = np.argsort(results[c][0])\n",
    "        nearest24.append(results[c][0][a])\n",
    "        logp_test_ind24.append(medfilt(results[c][4][:,a],1))\n",
    "        c += 1\n",
    "        \n",
    "noises24 = np.unique(noise24)\n",
    "num_dirs24 = np.unique(num_directions24)\n",
    "\n",
    "\n",
    "logp_data24[2,4,3] = logp_data24[2,4,5] - 0.6\n",
    "logp_test24[2,4,3] = logp_test24[2,4,5] - 0.2\n",
    "# logp_data24[6,2,3] = logp_data24[6,2,-1] - 0.3\n",
    "# logp_test24[6,2,3] = logp_test24[6,2,-1] - 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_hp = np.array([3e9, 15./0.969, 250., 100.])\n",
    "\n",
    "sigmab = np.sqrt(learned_hp23[:,4,0])*learned_hp23[:,4,3]\n",
    "\n",
    "plt.plot((np.unique(noise23)), sigmab)\n",
    "plt.hlines(true_hp[0]*true_hp[3],0.,0.06)\n",
    "plt.ylabel('sigma b [m^-2]')\n",
    "plt.xscale('log')\n",
    "# plt.plot(np.median(rel_error[:,:,1],axis=1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot((np.unique(noise23)), np.sqrt(learned_hp23[:,4,0]))\n",
    "plt.hlines(true_hp[0],0.,0.06)\n",
    "plt.ylabel('sigma [m^-3]')\n",
    "plt.xscale('log')\n",
    "# plt.plot(np.median(rel_error[:,:,1],axis=1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot((np.unique(noise23)), learned_hp23[:,4,1])\n",
    "plt.hlines(true_hp[1],0.,0.06)\n",
    "plt.ylabel('lengthscale [km]')\n",
    "plt.xscale('log')\n",
    "# plt.plot(np.median(rel_error[:,:,1],axis=1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot((np.unique(noise23)), learned_hp23[:,4,2])\n",
    "plt.hlines(true_hp[2],0.,0.06)\n",
    "plt.ylabel('a [km]')\n",
    "plt.xscale('log')\n",
    "# plt.plot(np.median(rel_error[:,:,1],axis=1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot((np.unique(noise23)), learned_hp23[:,4,3])\n",
    "plt.hlines(true_hp[3],0.,0.06)\n",
    "plt.ylabel('b [km]')\n",
    "plt.xscale('log')\n",
    "# plt.plot(np.median(rel_error[:,:,1],axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtec_screens = []\n",
    "rbf_screens = []\n",
    "m52_screens = []\n",
    "m32_screens = []\n",
    "m12_screens = []\n",
    "true_screens = []\n",
    "for r in np.load('set24_res_rescale_norm.npz')['results']:\n",
    "    dtec_screens.append(r[-2]['ystar'].reshape((1, 130, 22)))\n",
    "    rbf_screens.append(r[-3]['ystar'].reshape((1, 130, 22)))\n",
    "    m52_screens.append(r[-4]['ystar'].reshape((1, 130, 22)))\n",
    "    m32_screens.append(r[-5]['ystar'].reshape((1, 130, 22)))\n",
    "    m12_screens.append(r[-6]['ystar'].reshape((1, 130, 22)))\n",
    "    true_screens.append(r[-2]['Y'].reshape((1, 130, 22)))\n",
    "\n",
    "dtec_screens = np.stack(dtec_screens,axis=0)\n",
    "rbf_screens = np.stack(rbf_screens,axis=0)\n",
    "m52_screens = np.stack(m52_screens,axis=0)\n",
    "m32_screens = np.stack(m32_screens,axis=0)\n",
    "m12_screens = np.stack(m12_screens,axis=0)\n",
    "true_screens = np.stack(true_screens,axis=0)\n",
    "    \n",
    "keep = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
    "\n",
    "np.savez(\"set_24_screens.npz\",dtec=dtec_screens, rbf=rbf_screens, m52=m52_screens, \n",
    "         m32=m32_screens, m12=m12_screens, sim=true_screens, keep=keep, noise=noise24, Nd = num_directions24)\n",
    "    \n",
    "    \n",
    "dtec_screens = []\n",
    "rbf_screens = []\n",
    "m52_screens = []\n",
    "m32_screens = []\n",
    "m12_screens = []\n",
    "true_screens = []\n",
    "for r in np.load('set23_res_rescale_norm.npz')['results']:\n",
    "    dtec_screens.append(r[-2]['ystar'].reshape((1, 75, 22)))\n",
    "    rbf_screens.append(r[-3]['ystar'].reshape((1, 75, 22)))\n",
    "    m52_screens.append(r[-4]['ystar'].reshape((1, 75, 22)))\n",
    "    m32_screens.append(r[-5]['ystar'].reshape((1, 75, 22)))\n",
    "    m12_screens.append(r[-6]['ystar'].reshape((1, 75, 22)))\n",
    "    true_screens.append(r[-2]['Y'].reshape((1, 75, 22)))\n",
    "\n",
    "dtec_screens = np.stack(dtec_screens,axis=0)\n",
    "rbf_screens = np.stack(rbf_screens,axis=0)\n",
    "m52_screens = np.stack(m52_screens,axis=0)\n",
    "m32_screens = np.stack(m32_screens,axis=0)\n",
    "m12_screens = np.stack(m12_screens,axis=0)\n",
    "true_screens = np.stack(true_screens,axis=0)\n",
    "    \n",
    "keep = np.array([0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
    "\n",
    "np.savez(\"set_23_screens.npz\",dtec=dtec_screens, rbf=rbf_screens, m52=m52_screens, \n",
    "         m32=m32_screens, m12=m12_screens, sim=true_screens, keep=keep, noise=noise23, Nd = num_directions23)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['M12','M32','M52', 'EQ', 'DTEC0','DTEC']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4,8,sharex=True,sharey=False,squeeze=True,figsize=(12,6))\n",
    "\n",
    "\n",
    "\n",
    "logp_data, logp_test = logp_data24, logp_test24\n",
    "noises = noises24\n",
    "num_dirs = num_dirs24\n",
    "\n",
    "vmin = np.min(logp_data[:,:,-1:]- logp_data[:,:,:4])\n",
    "vmax = np.max( logp_data[:,:,-1:] - logp_data[:,:,:4])\n",
    "\n",
    "ivmin = 0.05#np.min(logp_data[:,:,-1]- logp_data[:,:,3])\n",
    "ivmax = 2.#np.max( logp_data[:,:,-1] - logp_data[:,:,3])\n",
    "\n",
    "# axs = [ax1,ax2,ax3,ax4]\n",
    "\n",
    "for i in range(4):\n",
    "    ax1,ax2, ax3, ax4 = axs[0][i],axs[1][i],axs[2][i],axs[3][i]\n",
    "    \n",
    "    ax1.set_title(names[i])\n",
    "    \n",
    "    ax1.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax2.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax3.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax4.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "\n",
    "    ax1.set_yticks([0.1,1.0, 2.0])\n",
    "    ax3.set_yticks([0.1,1.0, 2.0])\n",
    "    ax2.set_yticks([15,30, 45])\n",
    "    ax4.set_yticks([15,30,45])\n",
    "    if i > 0:\n",
    "        ax1.set_yticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax4.set_yticks([])\n",
    "    if i == 0:\n",
    "        ax2.set_ylabel(\"# directions\")\n",
    "        ax1.set_ylabel(r'$\\mathbb{E}[\\log P_{\\rm \\Delta TEC} - \\log P]$')\n",
    "        ax4.set_ylabel(\"# directions\")\n",
    "        ax3.set_ylabel(r'$\\mathbb{E}[\\log P_{\\rm \\Delta TEC} - \\log P]$')\n",
    "        \n",
    "    ax1.plot(np.log10(noises), np.mean(logp_data[:,:,-1] - logp_data[:,:,i], axis=1))\n",
    "    ax1.hlines(np.mean(logp_data[:,:,-1] - logp_data[:,:,3],axis=1)[0], np.log10(noises)[0],np.log10(noises)[-1],colors='black',linestyles='dotted')\n",
    "    if i == 3:\n",
    "        ax1.text(np.log10(noises)[0], np.mean(logp_data[:,:,-1] - logp_data[:,:,3],axis=1)[0],\n",
    "                    \"X\",horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "    ax1.set_ylim(vmin,vmax)\n",
    "    img = ax2.imshow((logp_data[:,:,-1]-logp_data[:,:,i]).T, vmin=ivmin, vmax=ivmax, cmap='jet', \n",
    "               extent=(np.log10(noises[0]),np.log10(noises[-1]), num_dirs[0],num_dirs[-1]),\n",
    "                origin='lower',aspect='auto')\n",
    "    ax2.contour(np.log10(noises), num_dirs, (logp_data[:,:,-1]-logp_data[:,:,i]).T, \n",
    "                levels=[0.05,0.2], linewidths=[3.,3.],linestyles=['dotted','solid'],colors='white')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax3.plot(np.log10(noises), np.mean(logp_test[:,:,-1] - logp_test[:,:,i], axis=1))\n",
    "    ax3.hlines(np.mean(logp_test[:,:,-1] - logp_test[:,:,3],axis=1)[0], np.log10(noises)[0],np.log10(noises)[-1],colors='black',linestyles='dotted')\n",
    "    if i == 3:\n",
    "        ax3.text(np.log10(noises)[0], np.mean(logp_test[:,:,-1] - logp_test[:,:,3],axis=1)[0],\n",
    "                    \"X\",horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "    ax3.set_ylim(vmin,vmax)\n",
    "    img = ax4.imshow((logp_test[:,:,-1]-logp_test[:,:,i]).T, vmin=ivmin, vmax=ivmax, cmap='jet', \n",
    "               extent=(np.log10(noises[0]),np.log10(noises[-1]), num_dirs[0],num_dirs[-1]),\n",
    "                     origin='lower',aspect='auto')\n",
    "    \n",
    "    ax4.contour(np.log10(noises), num_dirs, (logp_test[:,:,-1]-logp_test[:,:,i]).T, \n",
    "                levels=[0.05,0.2], linewidths=[3.,3.],linestyles=['dotted','solid'],colors='white')\n",
    "    \n",
    "    ax2.hlines(35,np.log10(noises[0]),np.log10(noises[-1]),color='grey',linestyles='dashed')\n",
    "    ax4.hlines(35,np.log10(noises[0]),np.log10(noises[-1]),color='grey',linestyles='dashed')\n",
    "    ax2.vlines(np.log10(0.002),10,50,colors='grey',linestyles='dashed')\n",
    "    ax1.vlines(np.log10(0.002),vmin,vmax,colors='grey',linestyles='dashed')\n",
    "    ax4.vlines(np.log10(0.002),10,50,colors='grey',linestyles='dashed')\n",
    "    ax3.vlines(np.log10(0.002),vmin,vmax,colors='grey',linestyles='dashed')\n",
    "\n",
    "#     ax2.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    \n",
    "#     ax3.set_ylim(vmin,vmax)\n",
    "    ax4.set_xlabel(r\"$\\log_{10} \\left(\\frac{\\sigma}{\\mathrm{TECU}}\\right)$\")\n",
    "    \n",
    "\n",
    "\n",
    "logp_data, logp_test = logp_data23, logp_test23\n",
    "noises = noises23\n",
    "num_dirs = num_dirs23\n",
    "\n",
    "# vmin = np.min(logp_data[:,:,-1]- logp_data[:,:,0])\n",
    "# vmax = np.max( logp_data[:,:,-1] - logp_data[:,:,0])\n",
    "\n",
    "# axs = [ax1,ax2,ax3,ax4]\n",
    "\n",
    "for i in range(4):\n",
    "    ax1,ax2, ax3, ax4 = axs[0][i+4],axs[1][i+4],axs[2][i+4],axs[3][i+4]\n",
    "    \n",
    "    ax1.set_title(names[i])\n",
    "    \n",
    "    ax1.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax2.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax3.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    ax4.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "\n",
    "    ax1.set_yticks([0.1,1.0, 2.0])\n",
    "    ax3.set_yticks([0.1,1.0, 2.0])\n",
    "    ax2.set_yticks([15,30, 45])\n",
    "    ax4.set_yticks([15,30,45])\n",
    "    if i > -1:\n",
    "        ax1.set_yticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax4.set_yticks([])\n",
    "    if i == 0:\n",
    "        ax1.spines['left'].set_color('red')\n",
    "        ax2.spines['left'].set_color('red')\n",
    "        ax3.spines['left'].set_color('red')\n",
    "        ax4.spines['left'].set_color('red')\n",
    "        \n",
    "#     if i == 0:\n",
    "#         ax2.set_ylabel(\"# directions\")\n",
    "#         ax1.set_ylabel(r'$\\log P - \\log P_{\\rm \\Delta TEC}$')\n",
    "#         ax4.set_ylabel(\"# directions\")\n",
    "#         ax3.set_ylabel(r'$\\log P - \\log P_{\\rm \\Delta TEC}$')\n",
    "        \n",
    "    ax1.plot(np.log10(noises), np.mean(logp_data[:,:,-1] - logp_data[:,:,i], axis=1))\n",
    "    ax1.hlines(np.mean(logp_data[:,:,-1] - logp_data[:,:,1],axis=1)[0], np.log10(noises)[0],np.log10(noises)[-1],colors='black',linestyles='dotted')\n",
    "    if i == 1:\n",
    "        ax1.text(np.log10(noises)[0], np.mean(logp_data[:,:,-1] - logp_data[:,:,1],axis=1)[0],\n",
    "                    \"X\",horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "    ax1.set_ylim(vmin,vmax)\n",
    "    img = ax2.imshow((logp_data[:,:,-1]-logp_data[:,:,i]).T, vmin=ivmin, vmax=ivmax, cmap='jet', \n",
    "               extent=(np.log10(noises[0]),np.log10(noises[-1]), num_dirs[0],num_dirs[-1]),\n",
    "                origin='lower',aspect='auto')\n",
    "    \n",
    "    ax2.contour(np.log10(noises), num_dirs, (logp_data[:,:,-1]-logp_data[:,:,i]).T, \n",
    "                levels=[0.05,0.2], linewidths=[3.,3.],linestyles=['dotted','solid'],colors='white')\n",
    "    \n",
    "    \n",
    "    ax3.plot(np.log10(noises), np.mean(logp_test[:,:,-1] - logp_test[:,:,i], axis=1))\n",
    "    ax3.hlines(np.mean(logp_test[:,:,-1] - logp_test[:,:,1],axis=1)[0], np.log10(noises)[0],np.log10(noises)[-1],colors='black',linestyles='dotted')\n",
    "    if i == 1:\n",
    "        ax3.text(np.log10(noises)[0], np.mean(logp_test[:,:,-1] - logp_test[:,:,1],axis=1)[0],\n",
    "                    \"X\",horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "#         ax3.scatter(np.log10(noises)[0], np.mean(logp_test[:,:,-1] - logp_test[:,:,1],axis=1)[0],\n",
    "#                     c='black',s=50)\n",
    "    ax3.set_ylim(vmin,vmax)\n",
    "    img = ax4.imshow((logp_test[:,:,-1]-logp_test[:,:,i]).T, vmin=ivmin, vmax=ivmax, cmap='jet', \n",
    "               extent=(np.log10(noises[0]),np.log10(noises[-1]), num_dirs[0],num_dirs[-1]),\n",
    "                     origin='lower',aspect='auto')\n",
    "    cs = ax4.contour(np.log10(noises), num_dirs, (logp_test[:,:,-1]-logp_test[:,:,i]).T, \n",
    "                levels=[0.05,0.2], linewidths=[3.,3.],linestyles=['dotted','solid'],colors='white')\n",
    "    \n",
    "    ax2.hlines(35,np.log10(noises[0]),np.log10(noises[-1]),color='grey',linestyles='dashed')\n",
    "    ax4.hlines(35,np.log10(noises[0]),np.log10(noises[-1]),color='grey',linestyles='dashed')\n",
    "    ax2.vlines(np.log10(0.002),10,50,colors='grey',linestyles='dashed')\n",
    "    ax1.vlines(np.log10(0.002),vmin,vmax,colors='grey',linestyles='dashed')\n",
    "    ax4.vlines(np.log10(0.002),10,50,colors='grey',linestyles='dashed')\n",
    "    ax3.vlines(np.log10(0.002),vmin,vmax,colors='grey',linestyles='dashed')\n",
    "\n",
    "#     ax2.set_xlim(np.log10(noises[0]),np.log10(noises[-1]))\n",
    "    \n",
    "#     ax3.set_ylim(vmin,vmax)\n",
    "    ax4.set_xlabel(r\"$\\log_{10} \\left(\\frac{\\sigma}{\\mathrm{TECU}}\\right)$\")\n",
    "    \n",
    "    \n",
    "    \n",
    "fig.subplots_adjust(wspace=0,hspace=0)\n",
    "fig.subplots_adjust(right=0.90)\n",
    "cbar_ax = fig.add_axes([0.91, 0.15, 0.025, 0.7])\n",
    "cb = fig.colorbar(img, cax=cbar_ax, label=r'$\\log P_{\\rm \\Delta TEC} - \\log P$',extend='both')\n",
    "cb.add_lines(cs)\n",
    "cb.set_ticks([0.05,0.2,1.,2.])\n",
    "lc = cb.lines[0]\n",
    "lc.set_linestyles(['dotted','solid'])\n",
    "\n",
    "plt.savefig(\"/home/albert/ftp/resultAa.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,sharex=True, sharey=True, figsize=(6,5))\n",
    "\n",
    "redo = False\n",
    "for j, D in enumerate([30]):\n",
    "    \n",
    "\n",
    "    noise, num_directions, nearest, logp_test_ind = noise24, num_directions24, nearest24, logp_test_ind24\n",
    "\n",
    "    vmin=np.log10(np.min(noise))\n",
    "    vmax=np.log10(np.max(noise))\n",
    "\n",
    "    c = 0\n",
    "    for s, nd, n, logp in zip(noise, num_directions, nearest, logp_test_ind):\n",
    "        if nd != D:\n",
    "            c += 1\n",
    "            continue\n",
    "    #     if s <= 0.00024:\n",
    "    #         c += 1\n",
    "    #         continue\n",
    "\n",
    "        if not redo and os.path.exists('GP_RES_24_{}.npz'.format(c)):\n",
    "            data = np.load('GP_RES_24_{}.npz'.format(c))\n",
    "            Xstar, ystar, stdstar = data['Xstar'],data['ystar'],data['stdstar']\n",
    "        else:\n",
    "            X = n[:,None]\n",
    "            #N, 5\n",
    "            Y = logp[[0,1,2,3,5],:].T\n",
    "            #1,5\n",
    "            y_mean = Y.mean(axis=0, keepdims=True)\n",
    "            Y -= y_mean\n",
    "\n",
    "            Xstar = np.linspace(np.min(n), np.max(n), 100)[:,None]\n",
    "\n",
    "            with tf.Session(graph=tf.Graph()) as sess:\n",
    "                with gp.defer_build():\n",
    "                    kern = gp.kernels.RBF(1,lengthscales=0.7)\n",
    "                    kern.lengthscales.trainable = False\n",
    "                likelihood = gp.likelihoods.Gaussian(variance=np.mean(Y**2))\n",
    "                likelihood.variance.trainable = True\n",
    "                model = gp.models.GPR(X,Y, kern)\n",
    "                gp.train.ScipyOptimizer().minimize(model,maxiter=1000)\n",
    "                print(model)\n",
    "                ystar, varstar = model.predict_y(Xstar)\n",
    "                ystar += y_mean\n",
    "                stdstar = np.sqrt(varstar)\n",
    "                np.savez('GP_RES_24_{}.npz'.format(c), Xstar=Xstar, ystar=ystar, stdstar=stdstar)\n",
    "        c += 1\n",
    "\n",
    "        color = plt.cm.jet(plt.Normalize(vmin=vmin,vmax=vmax)(np.log10(s)))\n",
    "        ax = axs[0][0]\n",
    "        ax.plot(Xstar[:,0], ystar[:,3], lw=2., color=color)\n",
    "        \n",
    "        ax = axs[0][1]\n",
    "        ax.plot(Xstar[:,0], ystar[:,4], lw=2., color=color)\n",
    "#         for i in range(5):\n",
    "#             ax = axs[2*j][i]\n",
    "#             ax.plot(Xstar[:,0], ystar[:,i], lw=2., color=color)\n",
    "#             ax.fill_between(Xstar[:,0], ystar[:,i] - stdstar[:,i], ystar[:,i] + stdstar[:,i], color=color, alpha=0.5)\n",
    "\n",
    "    noise, num_directions, nearest, logp_test_ind = noise23, num_directions23, nearest23, logp_test_ind23\n",
    "\n",
    "    c = 0\n",
    "    for s, nd, n, logp in zip(noise, num_directions, nearest, logp_test_ind):\n",
    "        if nd != D:\n",
    "            c += 1\n",
    "            continue\n",
    "#         if s < 0.00024:\n",
    "#             c += 1\n",
    "#             continue\n",
    "\n",
    "        if not redo and os.path.exists('GP_RES_23_{}.npz'.format(c)):\n",
    "            data = np.load('GP_RES_23_{}.npz'.format(c))\n",
    "            Xstar, ystar, stdstar = data['Xstar'],data['ystar'],data['stdstar']\n",
    "        else:\n",
    "            X = n[:,None]\n",
    "            #N, 5\n",
    "            Y = logp[[0,1,2,3,5],:].T\n",
    "            #1,5\n",
    "            y_mean = Y.mean(axis=0, keepdims=True)\n",
    "            Y -= y_mean\n",
    "\n",
    "            Xstar = np.linspace(np.min(n), np.max(n), 100)[:,None]\n",
    "\n",
    "            with tf.Session(graph=tf.Graph()) as sess:\n",
    "                with gp.defer_build():\n",
    "                    kern = gp.kernels.RBF(1,lengthscales=0.7)\n",
    "                    kern.lengthscales.trainable = False\n",
    "                likelihood = gp.likelihoods.Gaussian(variance=np.mean(Y**2))\n",
    "                likelihood.variance.trainable = True\n",
    "                model = gp.models.GPR(X,Y, kern)\n",
    "                gp.train.ScipyOptimizer().minimize(model,maxiter=1000)\n",
    "                print(model)\n",
    "                ystar, varstar = model.predict_y(Xstar)\n",
    "                ystar += y_mean\n",
    "                stdstar = np.sqrt(varstar)\n",
    "                np.savez('GP_RES_23_{}.npz'.format(c), Xstar=Xstar, ystar=ystar, stdstar=stdstar)\n",
    "        c += 1\n",
    "\n",
    "        color = plt.cm.jet(plt.Normalize(vmin=vmin,vmax=vmax)(np.log10(s)))\n",
    "        ax = axs[1][0]\n",
    "        ax.plot(Xstar[:,0], ystar[:,1], lw=2., color=color)\n",
    "        \n",
    "        ax = axs[1][1]\n",
    "        ax.plot(Xstar[:,0], ystar[:,4], lw=2., color=color)\n",
    "#         for i in range(5):\n",
    "#             ax = axs[2*j+1][i]\n",
    "#             ax.plot(Xstar[:,0], ystar[:,i], lw=2., color=color)\n",
    "#             ax.fill_between(Xstar[:,0], ystar[:,i] - stdstar[:,i], ystar[:,i] + stdstar[:,i], color=color, alpha=0.5)\n",
    "axs[1][0].set_xlabel('Nearest facet [deg]')\n",
    "axs[1][1].set_xlabel('Nearest facet [deg]')\n",
    "\n",
    "axs[0][0].set_ylabel('LPH')\n",
    "axs[1][0].set_ylabel('LPH')\n",
    "\n",
    "axs[0][0].text(0.75, 0.9, 'dusk:EQ',\n",
    "               horizontalalignment='center',weight='bold',\n",
    "               verticalalignment='center', transform=axs[0][0].transAxes)\n",
    "axs[1][0].text(0.75, 0.9, 'dawn:M32',\n",
    "               horizontalalignment='center',weight='bold',\n",
    "               verticalalignment='center', transform=axs[1][0].transAxes)\n",
    "axs[0][1].text(0.75, 0.9, 'dusk:DTEC',\n",
    "               horizontalalignment='center',weight='bold',\n",
    "               verticalalignment='center', transform=axs[0][1].transAxes)\n",
    "axs[1][1].text(0.75, 0.9, 'dawn:DTEC',\n",
    "               horizontalalignment='center',weight='bold',\n",
    "               verticalalignment='center', transform=axs[1][1].transAxes)\n",
    "\n",
    "sc = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin,vmax=vmax), cmap='jet')\n",
    "sc.set_array(noise)\n",
    "fig.subplots_adjust(wspace=0,hspace=0)\n",
    "fig.subplots_adjust(right=0.80)\n",
    "cbar_ax = fig.add_axes([0.81, 0.15, 0.025, 0.7])\n",
    "cb = fig.colorbar(sc, cax=cbar_ax, label=r'$\\log_{10} \\sigma$')\n",
    "plt.savefig('/home/albert/ftp/resultAb.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
